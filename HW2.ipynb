{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASSIGNMENT #2\n",
    "Niki Deeny\n",
    "Due Friday 2016-07-08 at 11AM CST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.0:\n",
    "\n",
    "1) How do you merge  two sorted  lists/arrays of records of the form [key, value]? Where is this  used in Hadoop MapReduce? [Hint within the shuffle]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 991,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mkdir Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 992,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Array1.dat\n"
     ]
    }
   ],
   "source": [
    "%%writefile Array1.dat\n",
    "1|hello\n",
    "Blabla|1,2,4,5\n",
    "3.6|Hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 993,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Array2.dat\n"
     ]
    }
   ],
   "source": [
    "%%writefile Array2.dat\n",
    "Woo|5,7,8\n",
    "4|yaay\n",
    "2|woot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 994,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MRJoin.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRJoin.py\n",
    "\n",
    "import sys, os, re\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRJoin(MRJob):\n",
    "\n",
    "    SORT_VALUES = True\n",
    "\n",
    "    def mapper(self, _, line):    \n",
    "        splits = line.rstrip(\"\\n\").split(\"|\")\n",
    "        key = splits[0]\n",
    "        yield key, splits\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        for value in values:\n",
    "            yield key, value\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRJoin.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 995,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "ignoring partitioner keyword arg (requires real Hadoop): 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
      "Creating temp directory /var/folders/r5/xnt6t6nd343fqlxyqjxqz3n50qpq8v/T/MRJoin.z084224.20160710.015241.328283\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /var/folders/r5/xnt6t6nd343fqlxyqjxqz3n50qpq8v/T/MRJoin.z084224.20160710.015241.328283/output...\n",
      "\"1\"\t[\"1\", \"hello\"]\n",
      "\"2\"\t[\"2\", \"woot\"]\n",
      "\"3.6\"\t[\"3.6\", \"Hi\"]\n",
      "\"4\"\t[\"4\", \"yaay\"]\n",
      "\"Blabla\"\t[\"Blabla\", \"1,2,4,5\"]\n",
      "\"Woo\"\t[\"Woo\", \"5,7,8\"]\n",
      "Removing temp directory /var/folders/r5/xnt6t6nd343fqlxyqjxqz3n50qpq8v/T/MRJoin.z084224.20160710.015241.328283...\n"
     ]
    }
   ],
   "source": [
    "!python MRJoin.py Array1.dat Array2.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) What is  a combiner function in the context of Hadoop? \n",
    "\n",
    "    A combiner function is used as an intermediate step to summarize the outputs of the mapper before it goes to the reducer.\n",
    "\n",
    "3) Give an example where it can be used and justify why it should be used in the context of this problem.\n",
    "\n",
    "    It is used when you want to decrease the number of records going to the reducer by summarizing data further in each worker node.\n",
    "\n",
    "4) What is the Hadoop shuffle?\n",
    "\n",
    "    The Hadoop Shuffle is the process of moving map outputs to the reducers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.1: Counters as a debugging aid (and for getting work done, but please use sparingly as they are heavy)\n",
    "\n",
    "Consumer complaints dataset: Use Counters to do EDA (exploratory data analysis and to monitor progress)\n",
    "Counters are lightweight objects in Hadoop that allow you to keep track of system progress in both the map and reduce stages of processing. By default, Hadoop defines a number of standard counters in \"groups\"; these show up in the jobtracker webapp, giving you information such as \"Map input records\", \"Map output records\", etc. \n",
    "\n",
    "While processing information/data using MapReduce job, it is a challenge to monitor the progress of parallel threads running across nodes of distributed clusters. Moreover, it is also complicated to distinguish between the data that has been processed and the data which is yet to be processed. The MapReduce Framework offers a provision of user-defined Counters, which can be effectively utilized to monitor the progress of data across nodes of distributed clusters.\n",
    "\n",
    "Use the Consumer Complaints  Dataset provide here to complete this question:\n",
    "\n",
    "     https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv?dl=0\n",
    "\n",
    "The consumer complaints dataset consists of diverse consumer complaints, which have been reported across the United States regarding various types of loans. The dataset consists of records of the form:\n",
    "\n",
    "Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "\n",
    "Here’s is the first few lines of the  of the Consumer Complaints  Dataset:\n",
    "\n",
    "Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "1114245,Debt collection,Medical,Disclosure verification of debt,Not given enough info to verify debt,FL,32219,Web,11/13/2014,11/13/2014,\"Choice Recovery, Inc.\",Closed with explanation,Yes,\n",
    "1114488,Debt collection,Medical,Disclosure verification of debt,Right to dispute notice not received,TX,75006,Web,11/13/2014,11/13/2014,\"Expert Global Solutions, Inc.\",In progress,Yes,\n",
    "1114255,Bank account or service,Checking account,Deposits and withdrawals,,NY,11102,Web,11/13/2014,11/13/2014,\"FNIS (Fidelity National Information Services, Inc.)\",In progress,Yes,\n",
    "1115106,Debt collection,\"Other (phone, health club, etc.)\",Communication tactics,Frequent or repeated calls,GA,31721,Web,11/13/2014,11/13/2014,\"Expert Global Solutions, Inc.\",In progress,Yes,\n",
    "\n",
    "#### User-defined Counters\n",
    "\n",
    "Now, let’s use MapReduce Counters to identify the number of complaints pertaining to debt collection, mortgage and other categories (all other categories get lumped into this one) in the consumer complaints dataset. Basically produce the distribution of the Product column in this dataset using counters (limited to 3 counters here).\n",
    "\n",
    "Hadoop offers Job Tracker, an UI tool to determine the status and statistics of all jobs. Using the job tracker UI, developers can view the Counters that have been created. Screenshot your  job tracker UI as your job completes and include it here. Make sure that your user defined counters are visible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 996,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0\n",
      "100 48.5M  100 48.5M    0     0  1243k      0  0:00:39  0:00:39 --:--:-- 1426k\n"
     ]
    }
   ],
   "source": [
    "!curl -L https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv?dl=0 > Consumer_Complaints.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 997,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
      "1114245,Debt collection,Medical,Disclosure verification of debt,Not given enough info to verify debt,FL,32219,Web,11/13/2014,11/13/2014,\"Choice Recovery, Inc.\",Closed with explanation,Yes,\n",
      "1114488,Debt collection,Medical,Disclosure verification of debt,Right to dispute notice not received,TX,75006,Web,11/13/2014,11/13/2014,\"Expert Global Solutions, Inc.\",In progress,Yes,\n",
      "1114255,Bank account or service,Checking account,Deposits and withdrawals,,NY,11102,Web,11/13/2014,11/13/2014,\"FNIS (Fidelity National Information Services, Inc.)\",In progress,Yes,\n",
      "1115106,Debt collection,\"Other (phone, health club, etc.)\",Communication tactics,Frequent or repeated calls,GA,31721,Web,11/13/2014,11/13/2014,\"Expert Global Solutions, Inc.\",In progress,Yes,\n",
      "1115890,Credit reporting,,Incorrect information on credit report,Information is not mine,FL,33461,Web,11/12/2014,11/13/2014,TransUnion,In progress,Yes,\n",
      "1114180,Debt collection,Credit card,Cont'd attempts collect debt not owed,Debt is not mine,CA,95035,Web,11/12/2014,11/13/2014,\"Expert Global Solutions, Inc.\",In progress,Yes,\n",
      "1114124,Consumer loan,Vehicle loan,Managing the loan or lease,,MI,48430,Fax,11/12/2014,11/13/2014,Ally Financial Inc.,In progress,Yes,\n",
      "1113211,Debt collection,Credit card,False statements or representation,Indicated committed crime not paying,MS,39056,Web,11/12/2014,11/12/2014,\"Premium Asset Services, LLC\",In progress,Yes,\n",
      "1112443,Consumer loan,Installment loan,Managing the loan or lease,,CA,94509,Web,11/12/2014,11/12/2014,Ally Financial Inc.,In progress,Yes,\n",
      "-------------------\n",
      "72917,Credit card,,Advertising and marketing,,CA,95252,Web,11/30/2011,12/02/2011,Citibank,Closed without relief,Yes,No\n",
      "2027,Credit card,,Credit determination,,CA,95823,Postal mail,11/30/2011,12/02/2011,Citibank,Closed without relief,Yes,No\n",
      "2034,Credit card,,Transaction issue,,VA,20170,Web,11/30/2011,12/02/2011,JPMorgan Chase,Closed without relief,Yes,Yes\n",
      "2037,Credit card,,Balance transfer,,MO,63117,Referral,11/30/2011,11/30/2011,U.S. Bancorp,Closed without relief,Yes,No\n",
      "2038,Credit card,,Credit card protection / Debt protection,,CA,95119,Referral,11/30/2011,11/30/2011,Bank of America,Closed without relief,Yes,No\n",
      "2042,Credit card,,Credit card protection / Debt protection,,IL,60156,Web,11/30/2011,12/01/2011,Citibank,Closed with relief,Yes,No\n",
      "2043,Credit card,,Credit reporting,,FL,33764,Referral,11/30/2011,11/30/2011,Bank of America,Closed without relief,Yes,No\n",
      "2044,Credit card,,Other fee,,CA,94605,Web,11/30/2011,12/01/2011,Bank of America,Closed with relief,Yes,No\n",
      "2045,Credit card,,Billing disputes,,CA,94305,Web,11/30/2011,12/01/2011,Citibank,Closed without relief,Yes,No\n",
      "2036,Credit card,,Cash advance,,UT,84523,Referral,11/30/2011,11/30/2011,Wells Fargo,Closed without relief,Yes,No\n"
     ]
    }
   ],
   "source": [
    "!head Consumer_Complaints.csv\n",
    "!echo \"-------------------\"\n",
    "!tail Consumer_Complaints.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 998,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Remove first row of csv (column names)\n",
    "!sed 1d Consumer_Complaints.csv > Consumer_Complaints_alt.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 999,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Counter.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Counter.py\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class Counter(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip().lower().split(\",\")\n",
    "        product = line[1]\n",
    "        if product in [\"debt collection\", \"mortgage\"]:\n",
    "            self.increment_counter('product', product, 1)\n",
    "        else:\n",
    "            self.increment_counter('product', \"other\", 1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    Counter().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1000,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /var/folders/r5/xnt6t6nd343fqlxyqjxqz3n50qpq8v/T/Counter.z084224.20160710.015348.752839\n",
      "Running step 1 of 1...\n",
      "Counters: 3\n",
      "\tproduct\n",
      "\t\tdebt collection=44372\n",
      "\t\tmortgage=125752\n",
      "\t\tother=142788\n",
      "Streaming final output from /var/folders/r5/xnt6t6nd343fqlxyqjxqz3n50qpq8v/T/Counter.z084224.20160710.015348.752839/output...\n",
      "Removing temp directory /var/folders/r5/xnt6t6nd343fqlxyqjxqz3n50qpq8v/T/Counter.z084224.20160710.015348.752839...\n"
     ]
    }
   ],
   "source": [
    "!python Counter.py Consumer_Complaints_alt.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.2: Analyze the performance of your Mappers, Combiners and Reducers using Counters\n",
    "\n",
    "\n",
    "\n",
    "For this brief study the Input file will be one record (the next line only): \n",
    "foo foo quux labs foo bar quux\n",
    "\n",
    "\n",
    "#### HW2.2 Part 1\n",
    "Perform a word count analysis of this single record dataset using a Mapper and Reducer based WordCount (i.e., no combiners are used here) using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing this word count job. The answer  should be 1 and 4 respectively. Please explain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1001,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting line.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile line.txt\n",
    "foo foo quux labs foo bar quux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1002,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting WordCount.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile WordCount.py\n",
    "from mrjob.job import MRJob\n",
    "import sys\n",
    "\n",
    "class MRWordFreqCount(MRJob):\n",
    "    def mapper(self, _, line):\n",
    "        # Counts the number of times mapper is called\n",
    "        self.increment_counter('group', 'Num_mapper_calls', 1)\n",
    "        for word in line.split(\" \"):\n",
    "            yield word.lower(), 1\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        # Counts the number of times reducer is called\n",
    "        self.increment_counter('group', 'Num_reducer_calls', 1)\n",
    "        yield word, sum(counts)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordFreqCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1003,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bar 1\n",
      "foo 3\n",
      "labs 1\n",
      "quux 2\n",
      "[{'group': {'Num_mapper_calls': 1, 'Num_reducer_calls': 4}}]\n"
     ]
    }
   ],
   "source": [
    "from WordCount import MRWordFreqCount\n",
    "mr_job = MRWordFreqCount(args=['line.txt'])\n",
    "\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "\n",
    "    # Print the wordcount output\n",
    "    for line in runner.stream_output():\n",
    "        key, value = mr_job.parse_output_line(line)\n",
    "        print key, value\n",
    "\n",
    "# Print the count of mapper/reducer usage\n",
    "print runner.counters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see above that the word count has completed, and the number of times the mapper is called is 1, whereas the number of times the reducer is called is 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW2.2 Part 2\n",
    "\n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper and Reducer based WordCount (i.e., no combiners used anywhere)  using user defined Counters to count up how many times the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1004,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting WordCount_CC.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile WordCount_CC.py\n",
    "from mrjob.job import MRJob\n",
    "import sys\n",
    "\n",
    "class MRWordFreqCount_CC(MRJob):\n",
    "    def mapper(self, _, line):\n",
    "        # Counts the number of times mapper is called\n",
    "        self.increment_counter('group', 'Num_mapper_calls', 1)\n",
    "        line = line.strip().lower().split(\",\")\n",
    "        issue = line[3]\n",
    "        for word in issue.split(\" \"): # Counting words in \"Issue\"\n",
    "            yield word.lower(), 1\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        # Counts the number of times reducer is called\n",
    "        self.increment_counter('group', 'Num_reducer_calls', 1)\n",
    "        yield word, sum(counts)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordFreqCount_CC.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1005,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'group': {'Num_mapper_calls': 312912, 'Num_reducer_calls': 172}}]\n"
     ]
    }
   ],
   "source": [
    "from WordCount_CC import MRWordFreqCount_CC\n",
    "mr_job = MRWordFreqCount_CC(args=['Consumer_Complaints_alt.csv'])\n",
    "\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "\n",
    "# Print the count of mapper/reducer usage\n",
    "print runner.counters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapper was called 312,912 times, Reducer was called 172 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW2.2 Part 3\n",
    "\n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper, Reducer, and standalone combiner (i.e., not an in-memory combiner) based WordCount using user defined Counters to count up how many time the mapper, combiner, reducer are called. What is the value of your user defined Mapper Counter, combiner counter, and Reducer Counter after completing your word count job. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1006,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting WordCount_CC_Combiner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile WordCount_CC_Combiner.py\n",
    "from mrjob.job import MRJob\n",
    "import sys\n",
    "\n",
    "class MRWordFreqCount_CC_Combiner(MRJob):\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        # Counts the number of times mapper is called\n",
    "        self.increment_counter('group', 'Num_mapper_calls', 1)\n",
    "        line = line.strip().lower().split(\",\")\n",
    "        issue = line[3]\n",
    "        for word in issue.split(\" \"): # Counting words in \"Issue\"\n",
    "            yield word.lower(), 1\n",
    "    \n",
    "    def combiner(self, word, counts):\n",
    "        self.increment_counter('group','Num_combiner_calls',1)\n",
    "        yield word, sum(counts)\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        # Counts the number of times reducer is called\n",
    "        self.increment_counter('group', 'Num_reducer_calls', 1)\n",
    "        yield word, sum(counts)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordFreqCount_CC_Combiner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1007,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'group': {'Num_combiner_calls': 319, 'Num_mapper_calls': 312912, 'Num_reducer_calls': 172}}]\n"
     ]
    }
   ],
   "source": [
    "from WordCount_CC_Combiner import MRWordFreqCount_CC_Combiner\n",
    "mr_job = MRWordFreqCount_CC_Combiner(args=['Consumer_Complaints_alt.csv'])\n",
    "\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "\n",
    "# Print the count of mapper/reducer usage\n",
    "print runner.counters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combiner was called 319 times.  Mapper calls and Reducer calls were unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.2.1:  \n",
    "\n",
    "Using a single reducer perform a sort of the words in decreasing order of word counts. Present the top 50 terms and their frequency. If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items). \n",
    "\n",
    "HINT: You will need a second MRStep for the sort part. Step 1 will be the usual word count, while step 2 will be a sort step. Please use the Hadoop/MRJob framework to perform the sort. Please do NOT use any of the built-in sorts  from  python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1008,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HW221_1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HW221_1.py\n",
    "from mrjob.job import MRJob\n",
    "import sys\n",
    "\n",
    "class MRWordFreqCount_CC_Combiner(MRJob):\n",
    "    \n",
    "    def jobconf(self):\n",
    "        orig_jobconf = super(MRWordFreqCount_CC_Combiner, self).jobconf()        \n",
    "        custom_jobconf = {\n",
    "            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapred.text.key.comparator.options': '-k1rn',\n",
    "            'mapred.reduce.tasks': '1',\n",
    "        }\n",
    "        combined_jobconf = orig_jobconf\n",
    "        combined_jobconf.update(custom_jobconf)\n",
    "        self.jobconf = combined_jobconf\n",
    "        return combined_jobconf\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip().lower().split(\",\")\n",
    "        issue = line[3]\n",
    "        for word in issue.split(\" \"): # Counting words in \"Issue\"\n",
    "            yield word.lower(), 1\n",
    "    \n",
    "    def combiner(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordFreqCount_CC_Combiner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1009,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from HW221_1 import MRWordFreqCount_CC_Combiner\n",
    "mr_job = MRWordFreqCount_CC_Combiner(args=['Consumer_Complaints_alt.csv'])\n",
    "wc={}\n",
    "\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "\n",
    "    for line in runner.stream_output():\n",
    "        key,value =  mr_job.parse_output_line(line)\n",
    "        wc[key] = value\n",
    "\n",
    "    with open('WordCount_Output.txt', 'w') as f:\n",
    "            for k in wc.keys():\n",
    "                f.writelines( k + \",\" + str(wc[k]) +\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1010,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HW221_Top50.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HW221_Top50.py\n",
    "from mrjob.job import MRJob\n",
    "import sys\n",
    "\n",
    "class HW221(MRJob):\n",
    "    #Sort\n",
    "    SORT_VALUES = True\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip().lower().split(\",\")\n",
    "        word, count = line\n",
    "        yield \"1\", (count, word)\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        thelist = []\n",
    "        order = 0\n",
    "        \n",
    "        for count, words in values:\n",
    "            order += 1\n",
    "            thelist.append((int(count),words,order))\n",
    "            thelist.sort(reverse=True) #Sort biggest to smallest\n",
    "        \n",
    "        # Show top 50\n",
    "        for k in thelist:\n",
    "            if k[2] >= int(order)-50:\n",
    "                print k[1],k[0]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    HW221.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1011,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "ignoring partitioner keyword arg (requires real Hadoop): 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
      "Creating temp directory /var/folders/r5/xnt6t6nd343fqlxyqjxqz3n50qpq8v/T/HW221_Top50.z084224.20160710.015533.040734\n",
      "Running step 1 of 1...\n",
      "modification 70487\n",
      "credit 50894\n",
      "problems 9484\n",
      "\"application 8625\n",
      "to 8401\n",
      "unable 8178\n",
      "billing 8158\n",
      "other 7886\n",
      "disputes 6938\n",
      "tactics 6920\n",
      "communication 6920\n",
      "reporting 6559\n",
      "lease 6337\n",
      "the 6248\n",
      "low 5663\n",
      "funds 5663\n",
      "caused 5663\n",
      "by 5663\n",
      "being 5663\n",
      "process 5505\n",
      "verification 5214\n",
      "disclosure 5214\n",
      "managing 5006\n",
      "investigation 4858\n",
      "company's 4858\n",
      "identity 4729\n",
      "account 4476\n",
      "card 4405\n",
      "charged 976\n",
      "for 929\n",
      "i 925\n",
      "didn't 925\n",
      "dispute 904\n",
      "fees 807\n",
      "expect 807\n",
      "shopping 672\n",
      "unsolicited 640\n",
      "issuance 640\n",
      "transfer 597\n",
      "balance 597\n",
      "scam 566\n",
      "issues 538\n",
      "amount 98\n",
      "payment 92\n",
      "credited 92\n",
      "convenience 75\n",
      "checks 75\n",
      "day 71\n",
      "amt 71\n",
      "incorrect/missing 64\n",
      "disclosures 64\n",
      "Streaming final output from /var/folders/r5/xnt6t6nd343fqlxyqjxqz3n50qpq8v/T/HW221_Top50.z084224.20160710.015533.040734/output...\n",
      "Removing temp directory /var/folders/r5/xnt6t6nd343fqlxyqjxqz3n50qpq8v/T/HW221_Top50.z084224.20160710.015533.040734...\n"
     ]
    }
   ],
   "source": [
    "!python HW221_Top50.py --jobconf mapred.reduce.tasks=1 WordCount_Output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1012,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HW221_Bottom10.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HW221_Bottom10.py\n",
    "from mrjob.job import MRJob\n",
    "import sys\n",
    "\n",
    "class HW221(MRJob):\n",
    "    #Sort\n",
    "    SORT_VALUES = True\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip().lower().split(\",\")\n",
    "        word, count = line\n",
    "        yield \"1\", (count, word)\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        thelist = []\n",
    "        order = 0\n",
    "        \n",
    "        for count, words in values:\n",
    "            order += 1\n",
    "            thelist.append((int(count),words,order))\n",
    "            thelist.sort() #Sort smallest to largest\n",
    "        \n",
    "        # Show Bottom 10\n",
    "        for k in thelist:\n",
    "            if k[2] >= int(order)-10:\n",
    "                print k[1],k[0]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    HW221.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1013,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "ignoring partitioner keyword arg (requires real Hadoop): 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
      "Creating temp directory /var/folders/r5/xnt6t6nd343fqlxyqjxqz3n50qpq8v/T/HW221_Bottom10.z084224.20160710.015540.144066\n",
      "Running step 1 of 1...\n",
      "credited 92\n",
      "payment 92\n",
      "amount 98\n",
      "dispute 904\n",
      "didn't 925\n",
      "i 925\n",
      "for 929\n",
      "charged 976\n",
      "to 8401\n",
      "\"application 8625\n",
      "problems 9484\n",
      "Streaming final output from /var/folders/r5/xnt6t6nd343fqlxyqjxqz3n50qpq8v/T/HW221_Bottom10.z084224.20160710.015540.144066/output...\n",
      "Removing temp directory /var/folders/r5/xnt6t6nd343fqlxyqjxqz3n50qpq8v/T/HW221_Bottom10.z084224.20160710.015540.144066...\n"
     ]
    }
   ],
   "source": [
    "!python HW221_Bottom10.py --jobconf mapred.reduce.tasks=1 WordCount_Output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.2.2:   \n",
    "Repeat HW2.2.1 using 3 reducers. Use the same code as in HW2.2.1  with just one modification \n",
    "to the command line: just add --jobconf mapred.reduce.tasks=3 as see presented here: \n",
    "\n",
    "    python HW2.2WordCount.py --jobconf mapred.reduce.tasks=3 oneLinerTextFile.txt\n",
    "\n",
    "Describe what you see. Is this correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1014,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HW222.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HW222.py\n",
    "from mrjob.job import MRJob\n",
    "import sys\n",
    "\n",
    "class MRWordFreqCount_CC_Combiner(MRJob):\n",
    "    \n",
    "    def jobconf(self):\n",
    "        orig_jobconf = super(MRWordFreqCount_CC_Combiner, self).jobconf()        \n",
    "        custom_jobconf = {\n",
    "            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapred.text.key.comparator.options': '-k1rn',\n",
    "            'mapred.reduce.tasks': '3', # change to 3 reducers\n",
    "        }\n",
    "        combined_jobconf = orig_jobconf\n",
    "        combined_jobconf.update(custom_jobconf)\n",
    "        self.jobconf = combined_jobconf\n",
    "        return combined_jobconf\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip().lower().split(\",\")\n",
    "        issue = line[3]\n",
    "        for word in issue.split(\" \"): # Counting words in \"Issue\"\n",
    "            yield word.lower(), 1\n",
    "    \n",
    "    def combiner(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordFreqCount_CC_Combiner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1015,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from HW222 import MRWordFreqCount_CC_Combiner\n",
    "mr_job = MRWordFreqCount_CC_Combiner(args=['Consumer_Complaints_alt.csv'])\n",
    "wc={}\n",
    "\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "\n",
    "    for line in runner.stream_output():\n",
    "        key,value =  mr_job.parse_output_line(line)\n",
    "        wc[key] = value\n",
    "\n",
    "    with open('WordCount_Output_222.txt', 'w') as f:\n",
    "            for k in wc.keys():\n",
    "                f.writelines( k + \",\" + str(wc[k]) +\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1016,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "ignoring partitioner keyword arg (requires real Hadoop): 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
      "Creating temp directory /var/folders/r5/xnt6t6nd343fqlxyqjxqz3n50qpq8v/T/HW221_Top50.z084224.20160710.015558.357419\n",
      "Running step 1 of 1...\n",
      "modification 70487\n",
      "credit 50894\n",
      "problems 9484\n",
      "\"application 8625\n",
      "to 8401\n",
      "unable 8178\n",
      "billing 8158\n",
      "other 7886\n",
      "disputes 6938\n",
      "tactics 6920\n",
      "communication 6920\n",
      "reporting 6559\n",
      "lease 6337\n",
      "the 6248\n",
      "low 5663\n",
      "funds 5663\n",
      "caused 5663\n",
      "by 5663\n",
      "being 5663\n",
      "process 5505\n",
      "verification 5214\n",
      "disclosure 5214\n",
      "managing 5006\n",
      "investigation 4858\n",
      "company's 4858\n",
      "identity 4729\n",
      "account 4476\n",
      "card 4405\n",
      "charged 976\n",
      "for 929\n",
      "i 925\n",
      "didn't 925\n",
      "dispute 904\n",
      "fees 807\n",
      "expect 807\n",
      "shopping 672\n",
      "unsolicited 640\n",
      "issuance 640\n",
      "transfer 597\n",
      "balance 597\n",
      "scam 566\n",
      "issues 538\n",
      "amount 98\n",
      "payment 92\n",
      "credited 92\n",
      "convenience 75\n",
      "checks 75\n",
      "day 71\n",
      "amt 71\n",
      "incorrect/missing 64\n",
      "disclosures 64\n",
      "Streaming final output from /var/folders/r5/xnt6t6nd343fqlxyqjxqz3n50qpq8v/T/HW221_Top50.z084224.20160710.015558.357419/output...\n",
      "Removing temp directory /var/folders/r5/xnt6t6nd343fqlxyqjxqz3n50qpq8v/T/HW221_Top50.z084224.20160710.015558.357419...\n"
     ]
    }
   ],
   "source": [
    "!python HW221_Top50.py --jobconf mapred.reduce.tasks=3 WordCount_Output_222.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1017,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "ignoring partitioner keyword arg (requires real Hadoop): 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
      "Creating temp directory /var/folders/r5/xnt6t6nd343fqlxyqjxqz3n50qpq8v/T/HW221_Bottom10.z084224.20160710.015602.249641\n",
      "Running step 1 of 1...\n",
      "credited 92\n",
      "payment 92\n",
      "amount 98\n",
      "dispute 904\n",
      "didn't 925\n",
      "i 925\n",
      "for 929\n",
      "charged 976\n",
      "to 8401\n",
      "\"application 8625\n",
      "problems 9484\n",
      "Streaming final output from /var/folders/r5/xnt6t6nd343fqlxyqjxqz3n50qpq8v/T/HW221_Bottom10.z084224.20160710.015602.249641/output...\n",
      "Removing temp directory /var/folders/r5/xnt6t6nd343fqlxyqjxqz3n50qpq8v/T/HW221_Bottom10.z084224.20160710.015602.249641...\n"
     ]
    }
   ],
   "source": [
    "!python HW221_Bottom10.py --jobconf mapred.reduce.tasks=3 WordCount_Output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "Seems to still be the same output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.3: Shopping Cart Analysis\n",
    "Product Recommendations: The action or practice of selling additional products or services \n",
    "to existing customers is called cross-selling. Giving product recommendation is \n",
    "one of the examples of cross-selling that are frequently used by online retailers. \n",
    "One simple method to give product recommendations is to recommend products that are frequently\n",
    "browsed together by the customers.\n",
    "  \n",
    "For this homework use the online browsing behavior dataset located at: \n",
    "\n",
    "       https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0\n",
    "\n",
    "Each line in this dataset represents a browsing session of a customer. \n",
    "On each line, each string of 8 characters represents the id of an item browsed during that session. \n",
    "The items are separated by spaces.\n",
    "\n",
    "Here are the first few lines of the ProductPurchaseData \n",
    "FRO11987 ELE17451 ELE89019 SNA90258 GRO99222 \n",
    "GRO99222 GRO12298 FRO12685 ELE91550 SNA11465 ELE26917 ELE52966 FRO90334 SNA30755 ELE17451 FRO84225 SNA80192 \n",
    "ELE17451 GRO73461 DAI22896 SNA99873 FRO86643 \n",
    "ELE17451 ELE37798 FRO86643 GRO56989 ELE23393 SNA11465 \n",
    "ELE17451 SNA69641 FRO86643 FRO78087 SNA11465 GRO39357 ELE28573 ELE11375 DAI54444 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1018,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 3377k  100 3377k    0     0   579k      0  0:00:05  0:00:05 --:--:--  986k\n"
     ]
    }
   ],
   "source": [
    "!curl -L https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0 > ProductPurchaseData.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1019,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FRO11987 ELE17451 ELE89019 SNA90258 GRO99222 \r\n",
      "GRO99222 GRO12298 FRO12685 ELE91550 SNA11465 ELE26917 ELE52966 FRO90334 SNA30755 ELE17451 FRO84225 SNA80192 \r\n",
      "ELE17451 GRO73461 DAI22896 SNA99873 FRO86643 \r\n",
      "ELE17451 ELE37798 FRO86643 GRO56989 ELE23393 SNA11465 \r\n",
      "ELE17451 SNA69641 FRO86643 FRO78087 SNA11465 GRO39357 ELE28573 ELE11375 DAI54444 \r\n",
      "ELE17451 GRO73461 DAI22896 SNA99873 FRO18919 DAI50921 SNA80192 GRO75578 \r\n",
      "ELE17451 ELE59935 FRO18919 ELE23393 SNA80192 SNA85662 SNA91554 DAI22177 \r\n",
      "ELE17451 SNA69641 FRO18919 SNA90258 ELE28573 ELE11375 DAI14125 FRO78087 \r\n",
      "ELE17451 GRO73461 DAI22896 SNA80192 SNA85662 SNA90258 DAI46755 FRO81176 ELE66810 DAI49199 DAI91535 GRO94758 ELE94711 DAI22177 \r\n",
      "ELE17451 SNA69641 DAI91535 GRO94758 GRO99222 FRO76833 FRO81176 SNA80192 DAI54690 ELE37798 GRO56989 \r\n"
     ]
    }
   ],
   "source": [
    "!head ProductPurchaseData.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some exploratory data analysis of this dataset guided by the following questions:. \n",
    "\n",
    "How many unique items are available from this supplier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1021,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12592"
      ]
     },
     "execution_count": 1021,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"ProductPurchaseData.txt\", \"rb\") as f:\n",
    "    data=f.read().strip().lower().split(\" \\n\")\n",
    "    line = [k.split(\" \") for k in data if k]\n",
    "\n",
    "allitems = []\n",
    "for k in line:\n",
    "    for i in k:\n",
    "        allitems.append(i)\n",
    "\n",
    "# Total unique number of items\n",
    "len(set(allitems))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a single reducer: Report your findings such as number of unique products; largest basket; report the top 50 most frequently purchased items,  using Hadoop Map-Reduce. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Largest Basket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1022,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HW23_LargestBakset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HW23_LargestBakset.py\n",
    "from mrjob.job import MRJob\n",
    "import sys\n",
    "\n",
    "class LargestBasket(MRJob):\n",
    "    #Sort\n",
    "    SORT_VALUES = True\n",
    "\n",
    "    def mapper(self, _, lines):\n",
    "        line=lines.strip().lower().split()\n",
    "        basket=len(line)\n",
    "        values = []\n",
    "        values.append(basket)\n",
    "        values.append(line)\n",
    "        yield \"key\", values\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        thelist = []\n",
    "        order = 0\n",
    "\n",
    "        for basket, line in values:\n",
    "            order += 1\n",
    "            thelist.append((int(basket),line,order))\n",
    "            thelist.sort(reverse=True) #Sort biggest to smallest\n",
    "        \n",
    "        print \"The Largest Basket has %s items in it. Here are the contents: %s\"  % (thelist[0][0],thelist[0][1])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    LargestBasket.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 990,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "ignoring partitioner keyword arg (requires real Hadoop): 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
      "Creating temp directory /var/folders/r5/xnt6t6nd343fqlxyqjxqz3n50qpq8v/T/HW23_LargestBakset.z084224.20160710.014734.069831\n",
      "Running step 1 of 1...\n",
      "The Largest Basket has 37 items in it. Here are the contents: ['gro21487', 'fro85978', 'dai89320', 'sna53220', 'sna55762', 'gro46854', 'ele38511', 'sna66583', 'fro79579', 'fro92469', 'fro40251', 'gro97448', 'dai35347', 'fro31317', 'fro87622', 'sna42518', 'ele53126', 'ele17451', 'gro32086', 'ele30327', 'dai58206', 'dai38969', 'ele16038', 'dai75645', 'dai55148', 'gro94173', 'ele43952', 'fro69613', 'gro81647', 'gro73461', 'fro24098', 'ele96667', 'gro88324', 'gro82670', 'gro12815', 'sna37475', 'ele24369']\n",
      "Streaming final output from /var/folders/r5/xnt6t6nd343fqlxyqjxqz3n50qpq8v/T/HW23_LargestBakset.z084224.20160710.014734.069831/output...\n",
      "Removing temp directory /var/folders/r5/xnt6t6nd343fqlxyqjxqz3n50qpq8v/T/HW23_LargestBakset.z084224.20160710.014734.069831...\n"
     ]
    }
   ],
   "source": [
    "!python HW23_LargestBakset.py ProductPurchaseData.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 50 Most Frequently Purhcased Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting HW23_1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile HW23_WC.py\n",
    "from mrjob.job import MRJob\n",
    "import sys\n",
    "\n",
    "class MRWordFreqCount_CC_Combiner(MRJob):\n",
    "    \n",
    "    def mapper(self, _, lines):\n",
    "        line=lines.strip().lower().split()\n",
    "        for word in line: # Counting words in \"Issue\"\n",
    "            yield word.lower(), 1\n",
    "    \n",
    "    def combiner(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        # Counts the number of times reducer is called\n",
    "        yield word, sum(counts)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordFreqCount_CC_Combiner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 893,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !python WordCount_CC_Combiner.py --jobconf mapred.reduce.tasks=1 Consumer_Complaints_alt.csv\n",
    "\n",
    "from HW23_WC import MRWordFreqCount_CC_Combiner\n",
    "mr_job = MRWordFreqCount_CC_Combiner(args=['ProductPurchaseData.txt'])\n",
    "wc={}\n",
    "\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "\n",
    "    for line in runner.stream_output():\n",
    "        key,value =  mr_job.parse_output_line(line)\n",
    "        wc[key] = value\n",
    "\n",
    "    with open('PP_Output.txt', 'w') as f:\n",
    "            for k in wc.keys():\n",
    "                f.writelines( k + \",\" + str(wc[k]) +\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "ignoring partitioner keyword arg (requires real Hadoop): 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
      "Creating temp directory /var/folders/r5/xnt6t6nd343fqlxyqjxqz3n50qpq8v/T/HW221_Top50.z084224.20160710.011836.942594\n",
      "Running step 1 of 1...\n",
      "gro81087 965\n",
      "dai55148 945\n",
      "dai91290 925\n",
      "gro24246 922\n",
      "gro64900 917\n",
      "sna18183 99\n",
      "gro38636 99\n",
      "fro61250 99\n",
      "sna97370 98\n",
      "sna87482 98\n",
      "sna63339 98\n",
      "fro49726 98\n",
      "fro47796 98\n",
      "gro94871 97\n",
      "gro90585 97\n",
      "gro66628 97\n",
      "ele93905 97\n",
      "ele73625 97\n",
      "ele67905 97\n",
      "ele40371 97\n",
      "fro73386 96\n",
      "sna18094 95\n",
      "gro92215 95\n",
      "ele31596 95\n",
      "ele24694 95\n",
      "dai51778 95\n",
      "sna79613 94\n",
      "sna30908 94\n",
      "gro54782 94\n",
      "gro16765 94\n",
      "fro91992 94\n",
      "fro73019 94\n",
      "sna90161 93\n",
      "sna37363 93\n",
      "sna28607 93\n",
      "dai71083 93\n",
      "dai46921 93\n",
      "sna79852 92\n",
      "sna48838 92\n",
      "gro73939 92\n",
      "ele76310 92\n",
      "ele24467 92\n",
      "ele12808 92\n",
      "dai50189 92\n",
      "dai20585 92\n",
      "dai20027 92\n",
      "sna53983 91\n",
      "sna35089 91\n",
      "sna12350 91\n",
      "gro88085 91\n",
      "fro56023 91\n",
      "Streaming final output from /var/folders/r5/xnt6t6nd343fqlxyqjxqz3n50qpq8v/T/HW221_Top50.z084224.20160710.011836.942594/output...\n",
      "Removing temp directory /var/folders/r5/xnt6t6nd343fqlxyqjxqz3n50qpq8v/T/HW221_Top50.z084224.20160710.011836.942594...\n"
     ]
    }
   ],
   "source": [
    "!python HW221_Top50.py --jobconf mapred.reduce.tasks=1 PP_Output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 OPTIONAL \n",
    "Using 2 reducers:  Report your findings such as number of unique products; largest basket; report the top 50 most frequently purchased items,  their frequency,  and their relative frequency (break ties by sorting the products alphabetical order) etc. using Hadoop Map-Reduce. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.4 (Computationally prohibitive but then again Hadoop can handle this) Pairs\n",
    "\n",
    "From a data mining perspective (and the aPriori algorihtm in particular), Support and Confidence are defined as follows:\n",
    "\n",
    "SUPPORT\n",
    "In data mining, the support value of X (where X is a collection of cooccurring items sometimes referred to as an item-set. E.g., a basket or subset of a basket) with respect to T  (a transaction database where each row is a transaction such as a basket of items that have been purchased)  is defined as the proportion of transactions in the  database which contains  the item-set X. (a relative frequency of sorts) \n",
    "\n",
    "CONFIDENCE \n",
    "The confidence value of a rule, X ==>  Y (where X is a collection of cooccurring items and Y is generally a single item. E.g., If Diapers and Beer then Cigars were also purchased), with respect to a set of transactions T, is the proportion of the transactions that contains X which also contains Y. (Think of it as  tgePr(Y|X) )\n",
    "\n",
    "The pairs/stripes algorithm returns cooccurrence information that can be used directly to  calculate the confidence and support. Note that confidence for pair X ==>  Y will  differ from the relative frequency that results from stripes when X occurs by itself in transactions.\n",
    "\n",
    "\n",
    "Suppose we want to recommend new products to the customer based on the products they\n",
    "have already browsed on the online website. Write a map-reduce program \n",
    "to find products which are frequently browsed together. Fix the support count (cooccurence count) to s = 100 \n",
    "(i.e. product pairs need to occur together at least 100 times to be considered frequent) \n",
    "and find pairs of items (sometimes referred to itemsets of size 2 in association rule mining) that have a support count of 100 or more.\n",
    "\n",
    "List the top 50 product pairs with corresponding support count (aka frequency), and relative frequency or support (number of records where they coccur, the number of records where they coccur/the number of baskets in the dataset)  in decreasing order of support  for frequent (100>count) itemsets of size 2. \n",
    "\n",
    "Use the Pairs pattern  to  extract these frequent itemsets of size 2. Free free to use combiners if they bring value. Instrument your code with counters for count the number of times your mapper, combiner and reducers are called.  \n",
    "\n",
    "Please output records of the following form for the top 50 pairs (itemsets of size 2): \n",
    "\n",
    "      item1, item2, support count, support  (OPTIONAL Feel free to add in confidence level also)\n",
    "\n",
    "\n",
    "\n",
    "Fix the ordering of the pairs lexicographically (left to right), \n",
    "and break ties in support (between pairs, if any exist) \n",
    "by taking the first ones in lexicographically increasing order. \n",
    "\n",
    "Report  the compute time for the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    "Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.5: Stripes\n",
    "Repeat 2.4 using the stripes design pattern for finding cooccuring pairs (and out.\n",
    "\n",
    "Report  the compute times for stripes job versus the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    "\n",
    "Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts. Discuss the differences in these counts between the Pairs and Stripes jobs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
