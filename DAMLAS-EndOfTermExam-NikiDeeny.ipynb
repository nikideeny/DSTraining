{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAMLAS MidTerm Exam  \n",
    "\n",
    "11:00AM - 1:00PM(CT)\n",
    "August 22, 2016   \n",
    "End of Term Exam\n",
    "\n",
    "\n",
    "Data Analytics and Machine Learning at Scale\n",
    "\n",
    "Target, Minneapolis\n",
    "\n",
    "### Please insert your contact information here\n",
    "Niki Deeny   \n",
    "niki.deeny@target.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions for exam \n",
    "\n",
    "0. Please submit your solutions and notebook via the following form:\n",
    "\n",
    "     [Exam Submission form](http://goo.gl/forms/iN4XIXQyiH1iAdyD3) \n",
    "\n",
    "            \n",
    "1. __Please acknowledge receipt of exam by sending a quick reply to the Jimi__ \n",
    "2. Review the submission form first to scope it out (it will take a 5-10 minutes to input your \n",
    "   answers and other information into this form)\n",
    "3. Please keep all your work and responses in ONE (1) notebook only (and submit via the submission form)\n",
    "4. Please make sure that the NBViewer link for your Submission notebook works\n",
    "5. Please do NOT discuss this exam with anyone (including your class mates) __until after Monday, September 5. __\n",
    "6. This is an open book exam meaning you can consult webpages and textbooks, class notes, slides etc. but you can not consult each other or any other person/group. Please complete this exam by yourself within the time limit. \n",
    "7. For markdown help in iPython Notebooks please see:\n",
    "https://sourceforge.net/p/ipython/discussion/markdown_syntax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exam questions begins here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from __future__ import division\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Setup for Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys #current as of 9/26/2015\n",
    "\n",
    "# spark_home = os.environ['SPARK_HOME'] = '/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/spark-1.6.1-bin-hadoop2.6/'\n",
    "spark_home = os.environ['SPARK_HOME'] = '/users/z084224/Downloads/spark-1.6.2-bin-hadoop2.6'\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME enviroment variable is not set')\n",
    "sys.path.insert(0,os.path.join(spark_home,'python'))\n",
    "sys.path.insert(0,os.path.join(spark_home,'python/lib/py4j-0.9-src.zip'))\n",
    "\n",
    "# First, we initialize the Spark environment\n",
    "\n",
    "# import findspark\n",
    "#findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# # We can give a name to our app (to find it in Spark WebUI) and configure execution mode\n",
    "# # In this case, it is local multicore execution with \"local[*]\"\n",
    "app_name = \"example-logs\"\n",
    "master = \"local[*]\"\n",
    "\n",
    "# Don't run this stuff twice\n",
    "conf = pyspark.SparkConf().setAppName(app_name).setMaster(master)\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "\n",
    "print(sc)\n",
    "print(sqlContext)\n",
    "\n",
    "\n",
    "# # Import some libraries to work with dates\n",
    "import dateutil.parser\n",
    "import dateutil.relativedelta as dateutil_rd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:1\n",
    "Assume your tasked with modeling a REGRESSION problem. How do you determine which variables may be important?\n",
    "\n",
    "\n",
    "1. If your data has unknown structure, start with:  Tree-based methods\n",
    "+ If statistical measures of importance are needed, start with: linear models (think Generalized linear models (GLMs)\n",
    "+ If statistical measures of importance are not needed, start with: Regression with shrinkage (e.g., LASSO, Elastic net)\n",
    "+ If statistical measures of importance are not needed, use : Stepwise regression\n",
    "\n",
    "Select the single most correct response from the following: \n",
    "\n",
    "+ (a) 1 \n",
    "+ (b) 2\n",
    "+ (c) 3\n",
    "+ (d) 1, 2, 3, 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**  \n",
    "D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:2\n",
    "Using one-hot-encoding, a categorical feature with four distinct values would be represented by how many features?\n",
    "\n",
    "(a) 1 feature  \n",
    "(b) 2 features  \n",
    "(c) 4 features  \n",
    "(d) none of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**  \n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:3\n",
    "When it comes to decision tree prediction (in the case of either classification or regression problems) which of the the following steps are perfermed:\n",
    "\n",
    "1. A decision tree takes as input an object or situation described by a set of attributes and returns a decision the predicted out value for the input.\n",
    "2. A decision tree reaches its decision by performing a sequence of tests.\n",
    "3. Each internal node in the tree corresponds to a test of the value of one of the properties, and the branches from the node are labeled with the possible values of the test.\n",
    "4. Each leaf node in the tree specifies the value to be returned if that leaf is reached.\n",
    "\n",
    "+ (a) 1 \n",
    "+ (b) 1, 2, 3, 4\n",
    "+ (c) 3\n",
    "+ (d) 1, 2, 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:  \n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:4\n",
    "\n",
    "Using Claude Shannon’s model of Entropy, compute the: \n",
    "\n",
    "+ Entropy of a fair coin\n",
    "+ Entropy of an unfair coint that comes up heads 80% of the time\n",
    "\n",
    "Select the correct answer from the following:\n",
    "\n",
    "+ (a) 1 bit (entropy of a fair coin) and 0.8 bits (the biased coin)\n",
    "+ (b) 1 bit (entropy of a fair coin) and 1 bit (the biased coin)\n",
    "+ (c) 1 bit (entropy of a fair coin) and 0.2068 bits (the biased coin)\n",
    "+ (d) 1 bit (entropy of a fair coin) and 0.2 bits (the biased coin)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: \n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:5\n",
    "Given the following gradient descent scenario indicate which of the statements below are correct: \n",
    "\n",
    " + Given an objective function (also know as a cost fucntion) that we wish to minimize. This function has one decision variable w1.\n",
    " + Suppose we reach a local minimum, i.e.,  w1 is already at a local minimum, what do you think one step of gradient descent will do? \n",
    " \n",
    " \n",
    " __STATEMENTS__\n",
    "\n",
    "1. It turns out at the local optimum, your derivative will be equal to zero. \n",
    "2. So  the slope of the line  tangent to the objective function at this point w1  will be equal to zero and thus this derivative term is equal to zero. \n",
    "3. The  gradient descent update at this local optimum is, w1 = w1 -  alpha * 0; where alpha is the learning rate \n",
    "4. If you're already at the local optimum it leaves  w1 unchanged cause its updates as w1 equals w1. So if your parameters are already at a local minimum one step with gradient descent does absolutely nothing. It keeps your solution at the local optimum. \n",
    "\n",
    "\n",
    "Select the single most correct answer from the following:\n",
    "\n",
    "+ (a) 1, 2, 3, 4\n",
    "+ (b) 2\n",
    "+ (c) 3,4\n",
    "+ (d) 2,3,4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:  \n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:6  \n",
    "\n",
    "Given the following gradient descent scenario indicate which of the statements below are correct: \n",
    "\n",
    " + Given a convex objective function (also known as a cost fucntion) that we wish to minimize. This function has one decision variable w1.\n",
    " + Suppose we reach a local minimum, i.e.,  w1 is already at a local minimum, what do you think one step of gradient descent will do? \n",
    " + use a fixed learning rate alpha; assume alpha is 1. \n",
    " \n",
    "__STATEMENTS__  \n",
    "a. When the current estimate of the minimum of the objective function is further away from the mimimum the gradient is much bigger   \n",
    "b. When the current estimate of the minimum of the objective function is close to the mimimum, my derivative term is even smaller and so the magnitude of the update to w1 is even smaller.  \n",
    "c. As gradient descent runs, you will automatically take smaller and smaller steps. Until eventually you're taking very small steps, and you finally converge to the  global minimum (it is a convex optimization after all).     \n",
    "d. Gradient descent will never converge will a fixed learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:  \n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:7\n",
    "When dealing with numercial data which of the following are ways to deal with missing data:\n",
    "\n",
    "(a) Delete records that have missing input values  \n",
    "(b) Standardize the data and set all missing values to 1 (one)  \n",
    "(c) Use K-nearest neighbours based on the test set to fill in missing values in the training set  \n",
    "(d) none of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:  \n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:8\n",
    "In a digital advertising problem where we are modeling the Pr(Click|context) and have no downstream knowledge of what purchases resulted from a click on the ad shown, which of the following approaches would lead to potentially more clicks on the ads shown?\n",
    "\n",
    "(a) Model Click vs not click event using linear regression   \n",
    "(b) Model Click vs not click event using logistic regression  \n",
    "(c) Model probability of a purchase  \n",
    "(d) none of the above  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:  \n",
    "D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:9\n",
    "Which of the following are true about the purpose of a loss function?\n",
    "\n",
    "(a) It’s a way to penalize a model for incorrect predictions  \n",
    "(b) It precisely defines the optimization problem to be solved for a particular learning model  \n",
    "(c) Loss functions can be used for modeling both classification and regression problems  \n",
    "(d) none of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:  \n",
    "A, B, C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:10\n",
    "When implementing Logistic (or linear) Regression with Regularization in Spark which of the following apply when using the following cost function: \n",
    "\n",
    "\\begin{equation*}\n",
    "minimize   \\left(f(w):= λR(w) + \\sum_{k=1}^n \\left(w;w_i,y_i \\right) \\right) \n",
    "\\end{equation*}\n",
    "\n",
    "(a) When lambda equals one, it provides the same result as standard logistic (linear) regression   \n",
    "(b) One only needs to modify the standard logistic (linear) regression (i.e., with no regularization term) by adding some code after the map-reduce (loss) gradient steps    \n",
    "(c) When lambda equals zero, it provides the same result as standard logistic (linear) regression  \n",
    "(d) One only needs to modify the standard logistic (linear) regression by modifying the mapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:  \n",
    "C, D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:11 Given the following paired RDDs \n",
    "RDD1 = {(1, 2), (3, 4), (3, 6)}\n",
    "RDD2 = {(3, 9) (3, 6)}\n",
    "\n",
    "Using PySpark, write code to perform an inner join of these paired RDDs. What is the resulting RDD? Make your Spark available in your notebook:\n",
    "\n",
    "a: [(3, (4, 9)), (3, (6, 9))]    \n",
    "b: [(3, (4, 9)), (3, (4, 6)), (3, (6, 9)), (3, (6, 6))]   \n",
    "c: [(3, (4, 9)), (3, (4, 6)), (3, (6, 9)), (3, (6, 9))]    \n",
    "d: None of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:  \n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, (4, 9))\n",
      "(3, (4, 6))\n",
      "(3, (6, 9))\n",
      "(3, (6, 6))\n"
     ]
    }
   ],
   "source": [
    "rdd1 =  sc.parallelize([(1,2), (3,4), (3,6)])\n",
    "rdd2 =  sc.parallelize([(3,9), (3,6)])\n",
    "\n",
    "join = rdd1.join(rdd2)\n",
    "for v in join.collect():\n",
    "    print v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:12  You have been tasked to build a predictive model to forecast beer sales for a chain of stores.\n",
    "\n",
    "\n",
    "After doing basic exploratory analysis on the data, what is the first thing you do regarding modeling?\n",
    "\n",
    "\n",
    "(a) Construct a baseline model  \n",
    "(b) Determine a metric to evaluate your machine learnt models  \n",
    "(c) Split your data into training, validation and test subsets (or split using cross fold validatation)  \n",
    "(d) All of the  of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:  \n",
    "D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:13\n",
    "\n",
    "The beer sales data consists of 52 weeks of cases-sold and price-per-case data for 3 carton sizes of beer (12-packs, 18-packs, 30-packs) at a small chain of supermarkets.\n",
    "\n",
    "Three additional rows of hypothetical price data for 12-,18-, 30-packs have been entered for purposes of forecasting from the models.  \n",
    "\n",
    "\n",
    "Use Spark and the following notebook, https://www.dropbox.com/s/6s5ph41h74bggwi/Linear-Regression-on-Beer-Data.ipynb?dl=0 to answer this question.\n",
    "\n",
    "The mean absolute percentage error (MAPE), also known as mean absolute percentage deviation (MAPD), is a measure of prediction accuracy of a model for say a forecasting method in statistics, \n",
    "for example in trend estimation. It usually expresses accuracy as a percentage, and is defined by the formula:\n",
    "\n",
    "MAPE = average over all examples (100*Abs(Actual - Predicted) / Actual)) \n",
    "\n",
    "Note when Actual is zero that test row is dropped from the evaluation.\n",
    "\n",
    "Construct a mean model for target variable `CASES18PK` (for the purposes of this question, take the mean of the CASES18PK column as your model prediction). Calculate the MAPE for the mean model over the training set. Select the closest answer to your calculated MAPE.\n",
    "\n",
    "(a) 200%  \n",
    "(b) 250%  \n",
    "(c) 20%  \n",
    "(d) 180%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer**:  \n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=sc.textFile(\"beerSales.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Week\tPRICE12PK\tPRICE18PK\tPRICE30PK\tCASES12PK\tCASES18PK\tCASES30PK\n",
      "1\t19.98\t14.10\t15.19\t223.5\t439\t55.00\n",
      "2\t19.98\t18.65\t15.19\t215.0\t98\t66.75\n",
      "3\t19.98\t18.65\t13.87\t227.5\t70\t242.00\n",
      "4\t19.98\t18.65\t12.83\t244.5\t52\t488.50\n",
      "5\t19.98\t18.65\t13.16\t313.5\t64\t308.75\n",
      "6\t19.98\t18.65\t15.19\t279.0\t72\t111.75\n",
      "7\t19.98\t18.65\t13.92\t238.0\t47\t252.50\n",
      "8\t20.10\t18.73\t14.42\t315.5\t85\t221.25\n",
      "9\t20.12\t18.75\t13.83\t217.0\t59\t245.25\n",
      "10\t20.13\t18.75\t14.50\t209.5\t63\t148.50\n",
      "11\t20.14\t18.75\t13.87\t227.0\t57\t229.75\n",
      "12\t20.12\t18.75\t13.64\t216.5\t54\t312.00\n",
      "13\t20.12\t13.87\t14.31\t169.0\t404\t96.75\n",
      "14\t20.13\t14.27\t13.85\t178.0\t380\t123.25\n",
      "15\t20.14\t18.76\t14.20\t301.5\t65\t200.50\n",
      "16\t20.14\t18.77\t13.64\t266.5\t40\t359.75\n",
      "17\t20.13\t13.87\t14.33\t182.5\t456\t113.50\n",
      "18\t20.13\t14.14\t13.14\t159.0\t176\t136.50\n",
      "19\t20.13\t18.76\t13.81\t285.5\t61\t225.50\n",
      "20\t20.13\t18.72\t15.19\t360.0\t91\t122.25\n",
      "21\t20.13\t18.76\t13.13\t263.0\t59\t443.75\n",
      "22\t19.18\t18.76\t13.63\t443.5\t83\t322.75\n",
      "23\t14.78\t18.74\t15.19\t1101.5\t41\t53.00\n",
      "24\t16.04\t18.75\t13.89\t814.0\t47\t140.75\n",
      "25\t20.12\t18.75\t14.28\t365.0\t84\t210.75\n",
      "26\t19.75\t18.75\t15.19\t510.0\t85\t110.50\n",
      "27\t19.65\t18.75\t13.12\t580.5\t116\t568.25\n",
      "28\t19.69\t13.79\t13.78\t251.0\t544\t115.50\n",
      "29\t20.12\t13.49\t15.19\t237.0\t890\t58.75\n",
      "30\t20.12\t14.89\t15.19\t302.5\t371\t77.25\n",
      "31\t20.13\t13.94\t15.19\t229.5\t557\t66.25\n",
      "32\t20.14\t13.67\t15.19\t188.5\t775\t50.00\n",
      "33\t15.14\t14.43\t15.19\t795.5\t236\t46.50\n",
      "34\t14.33\t18.75\t15.19\t1556.5\t43\t65.75\n",
      "35\t16.24\t18.22\t13.14\t807.5\t63\t252.75\n",
      "36\t19.93\t14.06\t13.45\t243.0\t469\t179.00\n",
      "37\t21.06\t14.43\t13.00\t201.5\t335\t226.25\n",
      "38\t21.19\t19.48\t13.60\t294.0\t75\t288.50\n",
      "39\t21.23\t15.15\t14.46\t220.5\t461\t114.25\n",
      "40\t20.12\t13.79\t14.94\t255.5\t817\t70.00\n",
      "41\t14.73\t14.31\t15.19\t920.5\t200\t47.75\n",
      "42\t14.57\t19.50\t15.19\t730.0\t32\t98.75\n",
      "43\t15.94\t13.85\t15.19\t262.5\t460\t77.00\n",
      "44\t20.70\t14.23\t13.43\t209.5\t751\t160.50\n",
      "45\t19.57\t19.31\t14.37\t283.0\t70\t143.50\n",
      "46\t19.60\t19.29\t15.19\t262.5\t80\t133.00\n",
      "47\t19.94\t13.76\t15.19\t310.0\t523\t68.75\n",
      "48\t21.28\t13.45\t15.19\t278.5\t741\t81.75\n",
      "49\t14.56\t15.13\t15.19\t741.5\t130\t56.25\n",
      "50\t14.39\t19.43\t15.19\t1316.0\t69\t68.75\n",
      "51\t16.81\t13.26\t15.19\t449.0\t493\t49.25\n",
      "52\t19.86\t13.92\t15.19\t505.0\t814\t76.50\n"
     ]
    }
   ],
   "source": [
    "for i in df.take(100):\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MAPE = average over all examples (100*Abs(Actual - Predicted) / Actual)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def linearRegressionGD(data, wInitial=None, learningRate=0.05, iterations=50):\n",
    "    featureLen = len(data.take(1)[0])-1\n",
    "    n = data.count()\n",
    "    if wInitial is None:  #start learning from a random vector\n",
    "        w = np.random.normal(size=featureLen) # w should be broadcasted if it is large\n",
    "    else:                 #start from provided vector\n",
    "        w = wInitial\n",
    "    for i in range(iterations):\n",
    "        wBroadcast = sc.broadcast(w)   #make available in memory as read-only to the executors (for mappers and reducers)\n",
    "        gradient = data.map(lambda d: -2 * (d[0] - np.dot(wBroadcast.value, d[1:])) * np.array(d[1:])) \\\n",
    "                    .reduce(lambda a, b: a + b)\n",
    "        w = w - learningRate * gradient/n\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:14\n",
    "Use Spark and the following notebook for the following questions:\n",
    "\n",
    "* http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/6s5ph41h74bggwi/Linear-Regression-on-Beer-Data.ipynb\n",
    "* https://www.dropbox.com/s/6s5ph41h74bggwi/Linear-Regression-on-Beer-Data.ipynb?dl=0 to answer this question.\n",
    "\n",
    "The target variable `CASES18P`K is skewed, so take the log of it (and make it more normally distributed) and compute the MAPE of the mean model for `LN_CASES18PK`. Select the closest answer to your calculated MAPE.\n",
    "\n",
    "(a) 200%  \n",
    "(b) 30%  \n",
    "(c) 20%  \n",
    "(d) 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:  \n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RDD' object has no attribute 'withColumn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-6f8460cafa2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIntegerType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdf_with_log\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"logCASES18PK\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CASES18PK\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_with_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RDD' object has no attribute 'withColumn'"
     ]
    }
   ],
   "source": [
    "rdd =sc.textFile(\"beerSales.txt\")\n",
    "\n",
    "# df = rdd.toDF(['Week','PRICE12PK','PRICE18PK','PRICE30PK','CASES12PK','CASES18PK','CASES30PK'])\n",
    "\n",
    "from math import log\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "df_with_log=df.withColumn(\"logCASES18PK\", log(\"CASES18PK\"))\n",
    "for v in df_with_log.take(100):\n",
    "    print v\n",
    "    \n",
    "# sqlContext.registerFunction(\"log\", lambda x: log(x))\n",
    "# sqlContext.registerDataFrameAsTable(df, \"df\")\n",
    "# sqlContext.sql(\"SELECT *, log(CASES18PK) as logCASES18PK FROM df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:15\n",
    "Use Spark and the following notebook for the following questions:\n",
    "\n",
    "* http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/6s5ph41h74bggwi/Linear-Regression-on-Beer-Data.ipynb\n",
    "* https://www.dropbox.com/s/6s5ph41h74bggwi/Linear-Regression-on-Beer-Data.ipynb?dl=0 to answer this question.\n",
    "\n",
    "\n",
    "Build a linear regression model using the following variables:\n",
    "\n",
    "Log(CASES18PK)  ~  log(PRICE12PK) + \tlog(PRICE18PK) +\tlog(PRICE30PK)\n",
    "\n",
    "Calculate MAPE over the test set and select the closest answer.\n",
    "\n",
    "(a) 4.3%   \n",
    "(b) 4.6%   \n",
    "(c) 3.5%  \n",
    "(d) 3.9%  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:  \n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'RDD' object has no attribute '__getitem__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-5ba8ed043df2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# create a regression friendly dataframe using patsy's dmatrices function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdmatrices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformula\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dataframe'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# instantiate our model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/z084224/anaconda/lib/python2.7/site-packages/patsy/highlevel.pyc\u001b[0m in \u001b[0;36mdmatrices\u001b[0;34m(formula_like, data, eval_env, NA_action, return_type)\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0meval_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEvalEnvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     (lhs, rhs) = _do_highlevel_design(formula_like, data, eval_env,\n\u001b[0;32m--> 297\u001b[0;31m                                       NA_action, return_type)\n\u001b[0m\u001b[1;32m    298\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlhs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mPatsyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model is missing required outcome variables\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/z084224/anaconda/lib/python2.7/site-packages/patsy/highlevel.pyc\u001b[0m in \u001b[0;36m_do_highlevel_design\u001b[0;34m(formula_like, data, eval_env, NA_action, return_type)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     design_infos = _try_incr_builders(formula_like, data_iter_maker, eval_env,\n\u001b[0;32m--> 152\u001b[0;31m                                       NA_action)\n\u001b[0m\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdesign_infos\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         return build_design_matrices(design_infos, data,\n",
      "\u001b[0;32m/Users/z084224/anaconda/lib/python2.7/site-packages/patsy/highlevel.pyc\u001b[0m in \u001b[0;36m_try_incr_builders\u001b[0;34m(formula_like, data_iter_maker, eval_env, NA_action)\u001b[0m\n\u001b[1;32m     55\u001b[0m                                       \u001b[0mdata_iter_maker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                                       \u001b[0meval_env\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                                       NA_action)\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/z084224/anaconda/lib/python2.7/site-packages/patsy/build.pyc\u001b[0m in \u001b[0;36mdesign_matrix_builders\u001b[0;34m(termlists, data_iter_maker, eval_env, NA_action)\u001b[0m\n\u001b[1;32m    694\u001b[0m                                                    \u001b[0mfactor_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m                                                    \u001b[0mdata_iter_maker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m                                                    NA_action)\n\u001b[0m\u001b[1;32m    697\u001b[0m     \u001b[0;31m# Now we need the factor infos, which encapsulate the knowledge of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0;31m# how to turn any given factor into a chunk of data:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/z084224/anaconda/lib/python2.7/site-packages/patsy/build.pyc\u001b[0m in \u001b[0;36m_examine_factor_types\u001b[0;34m(factors, factor_states, data_iter_maker, NA_action)\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_iter_maker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfactor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamine_needed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactor_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfactor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfactor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcat_sniffers\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mguess_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfactor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcat_sniffers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/z084224/anaconda/lib/python2.7/site-packages/patsy/eval.pyc\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, memorize_state, data)\u001b[0m\n\u001b[1;32m    564\u001b[0m         return self._eval(memorize_state[\"eval_code\"],\n\u001b[1;32m    565\u001b[0m                           \u001b[0mmemorize_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m                           data)\n\u001b[0m\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0m__getstate__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mno_pickling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/z084224/anaconda/lib/python2.7/site-packages/patsy/eval.pyc\u001b[0m in \u001b[0;36m_eval\u001b[0;34m(self, code, memorize_state, data)\u001b[0m\n\u001b[1;32m    549\u001b[0m                                  \u001b[0mmemorize_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eval_env\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m                                  \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                                  inner_namespace=inner_namespace)\n\u001b[0m\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmemorize_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhich_pass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/z084224/anaconda/lib/python2.7/site-packages/patsy/compat.pyc\u001b[0m in \u001b[0;36mcall_and_wrap_exc\u001b[0;34m(msg, origin, f, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_wrap_exc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morigin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/z084224/anaconda/lib/python2.7/site-packages/patsy/eval.pyc\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, expr, source_name, inner_namespace)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"eval\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         return eval(code, {}, VarLookupDict([inner_namespace]\n\u001b[0;32m--> 166\u001b[0;31m                                             + self._namespaces))\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Users/z084224/anaconda/lib/python2.7/site-packages/patsy/eval.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/z084224/anaconda/lib/python2.7/site-packages/patsy/eval.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'RDD' object has no attribute '__getitem__'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.nonparametric.kde import KDEUnivariate\n",
    "from statsmodels.nonparametric import smoothers_lowess\n",
    "from pandas import Series, DataFrame\n",
    "from patsy import dmatrices\n",
    "from sklearn import datasets, svm\n",
    "\n",
    "formula = 'CASES18PK ~ PRICE12PK + PRICE18PK + PRICE30PK' \n",
    "# create a results dictionary to hold our regression results for easy analysis later        \n",
    "results = {} \n",
    "\n",
    "# create a regression friendly dataframe using patsy's dmatrices function\n",
    "y,x = dmatrices(formula, data=df, return_type='dataframe')\n",
    "\n",
    "# instantiate our model\n",
    "# model = sm.Logit(y,x)\n",
    "\n",
    "# fit our model to the training data\n",
    "# res = model.fit()\n",
    "\n",
    "# save the result for outputing predictions later\n",
    "# results['Logit'] = [res, formula]\n",
    "# res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ET:16\n",
    "Recall that Spark automatically sends all variables referenced in your closures to the\n",
    "worker nodes. While this is convenient, it can also be inefficient because (1) the\n",
    "default task launching mechanism is optimized for small task sizes, and (2) you\n",
    "might, in fact, use the same variable in multiple parallel operations, but Spark will\n",
    "send it separately for each operation. As an example, say that we wanted to write a\n",
    "Spark program that looks up countries by their call signs (e.g., the call sign for Ireland is EJZ) by prefix matching in an\n",
    "table. In the following the \"signPrefixes\" variable is essentially a table with two columns \"Sign\" and \"Country Name\". The goal is \n",
    "to join the following tables:\n",
    "\n",
    "`signPrefixes` table with columns \"Sign\" and \"Country Name\"  \n",
    "`contactCounts` table with columns \"Sign\" and \"count\"\n",
    "\n",
    "to yield  a new table:\n",
    "\n",
    "`countryContactCounts` with the following columns \"Country Name\" and \"count\"\n",
    "\n",
    "Use Spark and the following notebook for the following questions:\n",
    "\n",
    "* http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/6s5ph41h74bggwi/Linear-Regression-on-Beer-Data.ipynb\n",
    "* https://www.dropbox.com/s/6s5ph41h74bggwi/Linear-Regression-on-Beer-Data.ipynb?dl=0 to answer this question.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 11.0 failed 1 times, most recent failure: Lost task 7.0 in stage 11.0 (TID 55, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/users/z084224/Downloads/spark-1.6.2-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/users/z084224/Downloads/spark-1.6.2-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/users/z084224/Downloads/spark-1.6.2-bin-hadoop2.6/python/pyspark/rdd.py\", line 2346, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/users/z084224/Downloads/spark-1.6.2-bin-hadoop2.6/python/pyspark/rdd.py\", line 2346, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/users/z084224/Downloads/spark-1.6.2-bin-hadoop2.6/python/pyspark/rdd.py\", line 317, in func\n    return f(iterator)\n  File \"/users/z084224/Downloads/spark-1.6.2-bin-hadoop2.6/python/pyspark/rdd.py\", line 1776, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/users/z084224/Downloads/spark-1.6.2-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/shuffle.py\", line 236, in mergeValues\n    for k, v in iterator:\n  File \"<ipython-input-33-b2b891b192b0>\", line 27, in <lambda>\n  File \"<ipython-input-33-b2b891b192b0>\", line 22, in processSignCount\n  File \"<ipython-input-33-b2b891b192b0>\", line 7, in lookupCountry\nNameError: global name 'bisect' is not defined\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:342)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:393)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/users/z084224/Downloads/spark-1.6.2-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/users/z084224/Downloads/spark-1.6.2-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/users/z084224/Downloads/spark-1.6.2-bin-hadoop2.6/python/pyspark/rdd.py\", line 2346, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/users/z084224/Downloads/spark-1.6.2-bin-hadoop2.6/python/pyspark/rdd.py\", line 2346, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/users/z084224/Downloads/spark-1.6.2-bin-hadoop2.6/python/pyspark/rdd.py\", line 317, in func\n    return f(iterator)\n  File \"/users/z084224/Downloads/spark-1.6.2-bin-hadoop2.6/python/pyspark/rdd.py\", line 1776, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/users/z084224/Downloads/spark-1.6.2-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/shuffle.py\", line 236, in mergeValues\n    for k, v in iterator:\n  File \"<ipython-input-33-b2b891b192b0>\", line 27, in <lambda>\n  File \"<ipython-input-33-b2b891b192b0>\", line 22, in processSignCount\n  File \"<ipython-input-33-b2b891b192b0>\", line 7, in lookupCountry\nNameError: global name 'bisect' is not defined\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:342)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-b2b891b192b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m                         .reduceByKey((lambda x, y: x + y)))\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcountryContactCounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# countryContactCounts.saveAsTextFile(outputDir + \"/countries.txt\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/z084224/Downloads/spark-1.6.2-bin-hadoop2.6/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/z084224/Downloads/spark-1.6.2-bin-hadoop2.6/python/pyspark/context.pyc\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m    937\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 939\u001b[0;31m         \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    940\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/z084224/Downloads/spark-1.6.2-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[0;32m--> 813\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/z084224/Downloads/spark-1.6.2-bin-hadoop2.6/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/z084224/Downloads/spark-1.6.2-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    306\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    307\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    309\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 11.0 failed 1 times, most recent failure: Lost task 7.0 in stage 11.0 (TID 55, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/users/z084224/Downloads/spark-1.6.2-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/users/z084224/Downloads/spark-1.6.2-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/users/z084224/Downloads/spark-1.6.2-bin-hadoop2.6/python/pyspark/rdd.py\", line 2346, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/users/z084224/Downloads/spark-1.6.2-bin-hadoop2.6/python/pyspark/rdd.py\", line 2346, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/users/z084224/Downloads/spark-1.6.2-bin-hadoop2.6/python/pyspark/rdd.py\", line 317, in func\n    return f(iterator)\n  File \"/users/z084224/Downloads/spark-1.6.2-bin-hadoop2.6/python/pyspark/rdd.py\", line 1776, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/users/z084224/Downloads/spark-1.6.2-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/shuffle.py\", line 236, in mergeValues\n    for k, v in iterator:\n  File \"<ipython-input-33-b2b891b192b0>\", line 27, in <lambda>\n  File \"<ipython-input-33-b2b891b192b0>\", line 22, in processSignCount\n  File \"<ipython-input-33-b2b891b192b0>\", line 7, in lookupCountry\nNameError: global name 'bisect' is not defined\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:342)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:393)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/users/z084224/Downloads/spark-1.6.2-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/users/z084224/Downloads/spark-1.6.2-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/users/z084224/Downloads/spark-1.6.2-bin-hadoop2.6/python/pyspark/rdd.py\", line 2346, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/users/z084224/Downloads/spark-1.6.2-bin-hadoop2.6/python/pyspark/rdd.py\", line 2346, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/users/z084224/Downloads/spark-1.6.2-bin-hadoop2.6/python/pyspark/rdd.py\", line 317, in func\n    return f(iterator)\n  File \"/users/z084224/Downloads/spark-1.6.2-bin-hadoop2.6/python/pyspark/rdd.py\", line 1776, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/users/z084224/Downloads/spark-1.6.2-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/shuffle.py\", line 236, in mergeValues\n    for k, v in iterator:\n  File \"<ipython-input-33-b2b891b192b0>\", line 27, in <lambda>\n  File \"<ipython-input-33-b2b891b192b0>\", line 22, in processSignCount\n  File \"<ipython-input-33-b2b891b192b0>\", line 7, in lookupCountry\nNameError: global name 'bisect' is not defined\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:342)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "#..... Other code...\n",
    "#Country lookup code\n",
    "\n",
    "## Helper functions for looking up the call signs\n",
    "\n",
    "def lookupCountry(sign, prefixes):\n",
    "    pos = bisect.bisect_left(prefixes, sign)\n",
    "    return prefixes[pos].split(\",\")[1]\n",
    "\n",
    "def loadCallSignTable():\n",
    "    f = open(\"callsign_tbl_sorted.txt\", \"r\")\n",
    "    return f.readlines()\n",
    "\n",
    "## Lookup the locations of the call signs on the\n",
    "## RDD contactCounts. We load a list of call sign\n",
    "## prefixes to country code to support this lookup.\n",
    "signPrefixes = loadCallSignTable()\n",
    "\n",
    "\n",
    "def processSignCount(sign_count, signPrefixes):\n",
    "    country = lookupCountry(sign_count[0], signPrefixes)\n",
    "    count = sign_count[1]\n",
    "    return (country, count)\n",
    "\n",
    "countryContactCounts = (contactCounts\n",
    "                        .map(lambda signCount: processSignCount(signCount, signPrefixes))\n",
    "                        .reduceByKey((lambda x, y: x + y)))\n",
    "\n",
    "for v in countryContactCounts.take(10):\n",
    "    print v\n",
    "# countryContactCounts.saveAsTextFile(outputDir + \"/countries.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we modfify this code to make it more efficient? Choose one response only\n",
    "\n",
    "(a) modify line 18 with `sc.broadcast(loadCallSignTable())`\n",
    "\n",
    "(b) Use accumulators to store the counts for each country  \n",
    "(c) The code is already optimal  \n",
    "(d) none of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:  \n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# End of Exam"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
