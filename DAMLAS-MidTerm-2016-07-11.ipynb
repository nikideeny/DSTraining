{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAMLAS MidTerm Exam  \n",
    "\n",
    "11:00AM - 1:00PM(CT)\n",
    "July 11, 2016   \n",
    "Midterm\n",
    "\n",
    "\n",
    "Data Analytics and Machine Learning at Scale\n",
    "\n",
    "Target, Minneapolis\n",
    "\n",
    "### Please insert your contact information here\n",
    "Niki Deeny  \n",
    "niki.deeny@target.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from __future__ import division\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exam Instructions\n",
    "\n",
    "1. : Please insert Name and Email address in the first cell of this notebook\n",
    "2. : Please acknowledge receipt of exam by sending a quick reply to exam email\n",
    "3. : Review the submission form first to scope it out (it will take a 5-10 minutes to input your \n",
    "   answers and other information into this form): \n",
    "\n",
    "    + [Exam Submission form](http://goo.gl/forms/iN4XIXQyiH1iAdyD3) \n",
    "\n",
    "4. : Please keep all your work and responses in ONE (1) notebook only (and submit via the submission form)\n",
    "5. : Please make sure that the NBViewer link for your Submission notebook works \n",
    "6. : Please submit your solutions and notebook via the following form:\n",
    "\n",
    "     + [Exam Submission form](http://goo.gl/forms/iN4XIXQyiH1iAdyD3)\n",
    "\n",
    "7. : For the midterm you will need access to MrJob and Jupyter on your local machines or on AWS to complete some of the questions (like fill in the code to do X).\n",
    "8. : As for question types:\n",
    "    + Knowledge test Programmatic/doodle (take photos; embed the photos in your notebook) \n",
    "    + All programmatic questions can be run locally on your laptop (using MrJob only) or on the cluster\n",
    "\n",
    "9. : This is an open book exam meaning you can consult webpages and textbooks, class notes, slides etc. but you can not each other or any other person/group. Please complete this exam by yourself within the time limit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exam questions begins here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===Map-Reduce==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MT1. Which of the following statememts about map-reduce are true? Check all that apply.   \n",
    "\n",
    "(a) If you only have 1 computer with 1 computing core, then map-reduce is unlikely to help   \n",
    "(b) If we run map-reduce using N computers, then we will always get at least an N-Fold speedup compared to using 1 computer   \n",
    "(c) Because of network latency and other overhead associated with map-reduce, if we run map-reduce using N    computers, then we will get less than N-Fold speedup compared to using 1 computer   \n",
    "(d) When using map-reduce with gradient descent, we usually use a single machine that accumulates the gradients from each of the map-reduce machines, in order to compute the paramter update for the iterion   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  MT1. a, b, c, d  # this is just a demo response\n",
    "<span style=\"font-weight:bold; color:green;\">__ANSWER__ This is Not necessary but it is always good way to cross check your logic. \n",
    "If you don't need the output to be sorted or aggregated, then there is no need for a reduce step. In other words, if all you're doing is applying some function to all data points and nothing else, then the reduce step is not needed.</SPAN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MT1. Answer\n",
    "A, B, D are true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===Order inversion===\n",
    "\n",
    "### MT2. Suppose you wish to write a MapReduce job that creates  normalized word co-occurrence data form a large input text. \n",
    "To ensure that all (potentially many) reducers\n",
    "receive appropriate normalization factors (denominators)\n",
    "in the correct order in their input streams\n",
    "(so as to minimize memory overhead), \n",
    "the mapper should emit according to which pattern:   \n",
    "\n",
    "(a) emit (*,word) count   \n",
    "(b) There is no need to use  order inversion here   \n",
    "(c) emit (word,*) count   \n",
    "(d) None of the above   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Answer MT2\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===Map-Reduce===\n",
    "\n",
    "### MT3. What is the input to the Reduce function in MRJob? Select the most correct choice.  \n",
    "\n",
    "   \n",
    "(a) An arbitrarily sized list of key/value pairs.    \n",
    "(b) One key and a list of some values associated with that key  \n",
    "(c) One key and a list of all values associated with that key.   \n",
    "(d) None of the above   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Answer MT3\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===Bayesian document classification===   \n",
    "  \n",
    "### MT4. When building a Bayesian document classifier, Laplace smoothing serves what purpose?   \n",
    "\n",
    "(a) It allows you to use your training data as your validation data.   \n",
    "(b) It prevents zero-products in the posterior distribution.  \n",
    "(c) It accounts for words that were missed by regular expressions.    \n",
    "(d) None of the above   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Answer MT4\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===Bias-variance tradeoff===   \n",
    "\n",
    "### MT5. By increasing the complexity of a model regressed on some samples of data,   it is likely that the ensemble will exhibit which of the following?   \n",
    "\n",
    "(a) Increased variance and bias   \n",
    "(b) Increased variance and decreased bias   \n",
    "(c) Decreased variance and bias   \n",
    "(d) Decreased variance and increased bias   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Answer MT5\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias: the power of the model is too simple, Variance: power of model is too high, when you overfit the data then the predictions vary greatly when the backend data changes (traning data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===Combiners===\n",
    "  \n",
    "### MT6. Combiners can be integral to the successful utilization of the Hadoop shuffle.  \n",
    "This utility is as a result of (select the most correct answer only)  \n",
    "\n",
    "(a) minimization of reducer workload \n",
    "(b) both (a) and (c)   \n",
    "(c) minimization of network traffic    \n",
    "(d) none of the above  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Answer MT6\n",
    "B: Both A and C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairwise similarity using K-L divergence\n",
    "\n",
    "In probability theory and information theory, the Kullback–Leibler divergence \n",
    "(also information divergence, information gain, relative entropy, KLIC, or KL divergence) \n",
    "is a non-symmetric measure of the difference between two probability distributions P and Q. \n",
    "Specifically, the Kullback–Leibler divergence of Q from P, denoted DKL(P\\‖Q), \n",
    "is a measure of the information lost when Q is used to approximate P:\n",
    "\n",
    "For discrete probability distributions P and Q, \n",
    "the Kullback–Leibler divergence of Q from P is defined to be\n",
    "\n",
    "    + KLDistance(P, Q) = Sum_over_item_i (P(i) log (P(i) / Q(i))      \n",
    "\n",
    "In the extreme cases, the KL Divergence is 1 when P and Q are maximally different\n",
    "and is 0 when the two distributions are exactly the same (follow the same distribution).\n",
    "\n",
    "For more information on K-L Divergence see:\n",
    "\n",
    "    + [K-L Divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)\n",
    "\n",
    "For the next three question we will use an MRjob class for calculating pairwise similarity \n",
    "using K-L Divergence as the similarity measure:\n",
    "\n",
    "Job 1: create inverted index (assume just two objects)\n",
    "Job 2: calculate/accumulate the similarity of each pair of objects using K-L Divergence\n",
    "\n",
    "Download the following notebook and then fill in the code for the first reducer to calculate \n",
    "the K-L divergence of objects (letter documents) in line1 and line2, i.e., KLD(Line1||line2).\n",
    "\n",
    "Here we ignore characters which are not alphabetical. And all alphabetical characters are lower-cased in the first mapper.\n",
    "\n",
    "http://nbviewer.ipython.org/urls/dl.dropbox.com/s/9onx4c2dujtkgd7/Kullback%E2%80%93Leibler%20divergence-MIDS-Midterm.ipynb\n",
    "https://www.dropbox.com/s/zr9xfhwakrxz9hc/Kullback%E2%80%93Leibler%20divergence-MIDS-Midterm.ipynb?dl=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the MRJob Class below  calculate the  KL divergence of the following two string objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting kltext.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile kltext.txt\n",
    "1.Data Science is an interdisciplinary field about processes and systems to extract knowledge or insights from large volumes of data in various forms (data in various forms, data in various forms, data in various forms), either structured or unstructured,[1][2] which is a continuation of some of the data analysis fields such as statistics, data mining and predictive analytics, as well as Knowledge Discovery in Databases.\n",
    "2.Machine learning is a subfield of computer science[1] that evolved from the study of pattern recognition and computational learning theory in artificial intelligence.[1] Machine learning explores the study and construction of algorithms that can learn from and make predictions on data.[2] Such algorithms operate by building a model from example inputs in order to make data-driven predictions or decisions,[3]:2 rather than following strictly static program instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.Data Science is an interdisciplinary field about processes and systems to extract knowledge or insights from large volumes of data in various forms (data in various forms, data in various forms, data in various forms), either structured or unstructured,[1][2] which is a continuation of some of the data analysis fields such as statistics, data mining and predictive analytics, as well as Knowledge Discovery in Databases.\\n2.Machine learning is a subfield of computer science[1] that evolved from the study of pattern recognition and computational learning theory in artificial intelligence.[1] Machine learning explores the study and construction of algorithms that can learn from and make predictions on data.[2] Such algorithms operate by building a model from example inputs in order to make data-driven predictions or decisions,[3]:2 rather than following strictly static program instructions.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(\"kltext.txt\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MRjob class for calculating pairwise similarity using K-L Divergence as the similarity measure\n",
    "\n",
    "Job 1: create inverted index (assume just two objects) <P>\n",
    "Job 2: calculate the similarity of each pair of objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0986122886681098"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.log(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.Data Science is an interdisciplinary field about processes and systems to extract knowledge or insights from large volumes of data in various forms (data in various forms, data in various forms, data in various forms), either structured or unstructured,[1][2] which is a continuation of some of the data analysis fields such as statistics, data mining and predictive analytics, as well as Knowledge Discovery in Databases.\r\n",
      "2.Machine learning is a subfield of computer science[1] that evolved from the study of pattern recognition and computational learning theory in artificial intelligence.[1] Machine learning explores the study and construction of algorithms that can learn from and make predictions on data.[2] Such algorithms operate by building a model from example inputs in order to make data-driven predictions or decisions,[3]:2 rather than following strictly static program instructions."
     ]
    }
   ],
   "source": [
    "!cat kltext.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting kldivergence.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile kldivergence.py\n",
    "from __future__ import division\n",
    "from mrjob.job import MRJob\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from math import log\n",
    "\n",
    "class kldivergence(MRJob):\n",
    "    \n",
    "    # process each string character by character\n",
    "    # the relative frequency of each character emitting Pr(character|str)\n",
    "    # for input record 1.abcbe\n",
    "    # emit \"a\"    [1, 0.2]\n",
    "    # emit \"b\"    [1, 0.4] etc...\n",
    "    def mapper1(self, _, line):\n",
    "        index = int(line.split('.',1)[0])\n",
    "        letter_list = re.sub(r\"[^A-Za-z]+\", '', line).lower()\n",
    "        count = {}\n",
    "        \n",
    "        for l in letter_list:\n",
    "            if count.has_key(l):\n",
    "                count[l] += 1\n",
    "            else:\n",
    "                count[l] = 1\n",
    "        for key in count:\n",
    "            yield key, [index, count[key]*1.0/len(letter_list)]\n",
    "            #KEY = the letter\n",
    "            #INDEX = the which sentense its from\n",
    "            #COUNT[key]*1.0 = the number of times it shows up in each\n",
    "            #len(letter_list) number of letters (not unique) in each sentence\n",
    "            #so it is outputting the letter, the sentence, and the likelihood of that letter showing up in that sentence\n",
    "#             print \"From mapper!\", key, [index, count[key]*1.0/len(letter_list)]\n",
    "\n",
    "    def reducer1(self, key, values):\n",
    "        p = 0\n",
    "        q = 0\n",
    "\n",
    "        for v in values:\n",
    "            if v[0] == 1:  #String 1\n",
    "                p = v[1]\n",
    "            else:          # String 2\n",
    "                q = v[1]\n",
    "    \n",
    "        yield \"key\", (p*log(p/q)) # K-L Distance\n",
    "        print \"KL Divergence per letter\", key, (p*log(p/q))\n",
    "\n",
    "    #Aggegate components            \n",
    "    def reducer2(self, key, values):\n",
    "        kl_sum = 0\n",
    "        for value in values:\n",
    "            kl_sum += value\n",
    "        yield \"KLDivergence\", kl_sum\n",
    "            \n",
    "    def steps(self):\n",
    "        return [self.mr(mapper=self.mapper1,\n",
    "                        reducer=self.reducer1),\n",
    "                \n",
    "                self.mr(reducer=self.reducer2)\n",
    "               \n",
    "               ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    kldivergence.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Divergence per letter a 0.0295721422713\n",
      "KL Divergence per letter b -0.00163041522831\n",
      "KL Divergence per letter c -0.00732786747342\n",
      "KL Divergence per letter d 0.0164906236566\n",
      "KL Divergence per letter e -0.0129926189574\n",
      "KL Divergence per letter f 0.00674079918689\n",
      "KL Divergence per letter g -0.00826965428728\n",
      "KL Divergence per letter h -0.00992358514474\n",
      "KL Divergence per letter i 0.00373655435066\n",
      "KL Divergence per letter k 0.000733812807303\n",
      "KL Divergence per letter l -0.0134916702888\n",
      "KL Divergence per letter m -0.00829112158145\n",
      "KL Divergence per letter n -0.021708593752\n",
      "KL Divergence per letter o -0.00910212088756\n",
      "KL Divergence per letter p -0.0094296551709\n",
      "KL Divergence per letter r -0.0071047011805\n",
      "KL Divergence per letter s 0.0907342592609\n",
      "KL Divergence per letter t -0.0102420842309\n",
      "KL Divergence per letter u 0.0147136183439\n",
      "KL Divergence per letter v 0.0198601378947\n",
      "KL Divergence per letter w 0.0176343237035\n",
      "KL Divergence per letter x -0.00165393085746\n",
      "KL Divergence per letter y 0.00183453201826\n",
      "('KLDivergence', 0.08088278445318145)\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from mrjob.job import MRJob\n",
    "from kldivergence import kldivergence\n",
    "\n",
    "#dont forget to save kltext.txt (see earlier cell)\n",
    "mr_job = kldivergence(args=['kltext.txt'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MT7. Which number below is the closest to the result you get for KLD(Line1||line2)?   \n",
    "(a) 0.7   \n",
    "(b) 0.5   \n",
    "(c) 0.2   \n",
    "(d) 0.1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Anser MT7\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MT8. Which of the following letters are missing from these character vectors?   \n",
    "(a) p and t   \n",
    "(b) k and q   \n",
    "(c) j and q   \n",
    "\n",
    "(d) j and f   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Answer MT8\n",
    "C: J and Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting kldivergence_smooth.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile kldivergence_smooth.py\n",
    "from __future__ import division\n",
    "from mrjob.job import MRJob\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from math import log\n",
    "\n",
    "class kldivergence_smooth(MRJob):\n",
    "    \n",
    "    # process each string character by character\n",
    "    # the relative frequency of each character emitting Pr(character|str)\n",
    "    # for input record 1.abcbe\n",
    "    # emit \"a\"    [1, (1+1)/(5+24)]\n",
    "    # emit \"b\"    [1, (2+1)/(5+24) etc...\n",
    "    def mapper1(self, _, line):\n",
    "        index = int(line.split('.',1)[0])\n",
    "        letter_list = re.sub(r\"[^A-Za-z]+\", '', line).lower()\n",
    "        count = {}\n",
    "        \n",
    "        # (ni+1)/(n+24)\n",
    "        \n",
    "        for l in letter_list:\n",
    "            if count.has_key(l):\n",
    "                count[l] += 1\n",
    "            else:\n",
    "                count[l] = 1\n",
    "        for key in count:\n",
    "            yield key, [index, (count[key]*1.0+1)/(len(letter_list)+24)] # Add the smoothing\n",
    "\n",
    "    \n",
    "    def reducer1(self, key, values):\n",
    "        p = 0\n",
    "        q = 0\n",
    "        for v in values:\n",
    "            if v[0] == 1:\n",
    "                p = v[1]\n",
    "            else:\n",
    "                q = v[1]\n",
    "                \n",
    "        yield \"key\", (p*log(p/q)) # K-L Distance\n",
    "        print \"KL Divergence per letter\", key, (p*log(p/q))\n",
    "\n",
    "    # Aggregate components             \n",
    "    def reducer2(self, key, values):\n",
    "        kl_sum = 0\n",
    "        for value in values:\n",
    "            kl_sum = kl_sum + value\n",
    "        yield \"KLDivergence\", kl_sum\n",
    "            \n",
    "    def steps(self):\n",
    "        return [self.mr(mapper=self.mapper1,\n",
    "                        reducer=self.reducer1),\n",
    "                self.mr(reducer=self.reducer2)\n",
    "               \n",
    "               ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    kldivergence_smooth.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Divergence per letter a 0.0271285770855\n",
      "KL Divergence per letter b -0.00138634600173\n",
      "KL Divergence per letter c -0.00693173000866\n",
      "KL Divergence per letter d 0.0152918078694\n",
      "KL Divergence per letter e -0.0124771140156\n",
      "KL Divergence per letter f 0.00639606313847\n",
      "KL Divergence per letter g -0.00797902003265\n",
      "KL Divergence per letter h -0.00955497069346\n",
      "KL Divergence per letter i 0.00310616930209\n",
      "KL Divergence per letter k 0.000965278568718\n",
      "KL Divergence per letter l -0.0128416309164\n",
      "KL Divergence per letter m -0.00783050035972\n",
      "KL Divergence per letter n -0.0206773746193\n",
      "KL Divergence per letter o -0.00877209353032\n",
      "KL Divergence per letter p -0.00973858492395\n",
      "KL Divergence per letter r -0.00687026526917\n",
      "KL Divergence per letter s 0.0835168653455\n",
      "KL Divergence per letter t -0.00991976510841\n",
      "KL Divergence per letter u 0.0137042758151\n",
      "KL Divergence per letter v 0.0176835515088\n",
      "KL Divergence per letter w 0.0140923225719\n",
      "KL Divergence per letter x -0.00156610007194\n",
      "KL Divergence per letter y 0.00193055713744\n",
      "('KLDivergence', 0.06726997279170038)\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from kldivergence_smooth import kldivergence_smooth\n",
    "mr_job = kldivergence_smooth(args=['kltext.txt'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MT9. The KL divergence on multinomials is defined only when they have nonzero entries. \n",
    "For zero entries, we have to smooth distributions. Suppose we smooth in this way:    \n",
    "\n",
    "(ni+1)/(n+24)   \n",
    "\n",
    "where ni is the count for letter i and n is the total count of all letters.    \n",
    "After smoothing, which number below is the closest to the result you get for KLD(Line1||line2)??   \n",
    "\n",
    "(a) 0.08   \n",
    "(b) 0.71     \n",
    "(c) 0.02    \n",
    "(d) 0.11   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Answer MT9\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===Gradient descent===\n",
    "\n",
    "### MT10. Which of the following are true statements with respect to gradient descent for machine learning, where alpha is the learning rate. Select all that apply   (stretch question)\n",
    "\n",
    " (a) To make gradient descent converge, we must slowly decrease alpha over time and use a combiner in the context of Hadoop.   \n",
    " (b) Gradient descent is guaranteed to find the global minimum for any function J() regardless of using a combiner or not in the context of Hadoop   \n",
    " (c) Gradient descent can converge even if alpha is kept fixed. (But alpha cannot be too large, or else it may fail to converge.) Combiners will help speed up the process.   \n",
    " (d) For the specific choice of cost function J() used in linear regression, there is no local optima (other than the global optimum).   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Answer MT10\n",
    "B, C, D are true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted K-means\n",
    "\n",
    "Below is MRJob code  for computing the KMeans of a 2-dimensional input data set. Modify this  MRJob to do the training at scale of a weighted K-means algorithm.\n",
    "\n",
    "Weight each example as follows using the inverse vector length (Euclidean norm): \n",
    "\n",
    "    + weight(X)= 1/||X||, \n",
    "\n",
    "where ||X|| = SQRT(X.X)= SQRT(x1^2 + x2^2)\n",
    "\n",
    "The weighted mean of a set of numbers of the form (x1 , x2), with corresponding weights (w1 , w2)  is computed from the following formula:\n",
    "\n",
    "    + weightedMean = (w1 * x1 + w2 * x2) / (w1 + w2) \n",
    "\n",
    "Here X is vector made up of x1 and x2. For more information on the weighted mean, see https://en.wikipedia.org/wiki/Weighted_arithmetic_mean\n",
    "\n",
    "Using the following data answer the following questions (download and save to this directory):\n",
    "\n",
    "   + https://www.dropbox.com/s/ai1uc3q2ucverly/Kmeandata.csv?dl=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that the data is available in the local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head Kmeandata.csv > head.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MRKmeansIteration.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRKmeansIteration.py\n",
    "from numpy import argmin, array, random\n",
    "from math import sqrt   \n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import os\n",
    "\n",
    "#Calculate find the nearest centroid for data point \n",
    "def MinDist(datapoint, centroid_points):\n",
    "    datapoint = array(datapoint)\n",
    "    centroid_points = array(centroid_points)\n",
    "    diff = datapoint - centroid_points \n",
    "    diffsq = diff*diff\n",
    "    # Get the nearest centroid for each instance\n",
    "    minidx = argmin(list(diffsq.sum(axis = 1)))\n",
    "    return minidx\n",
    "\n",
    "\n",
    "class MRKmeansIteration(MRJob):\n",
    "    k=3    \n",
    "    def steps(self):\n",
    "        return [MRStep(mapper_init = self.mapper_init, \n",
    "                       mapper=self.mapper,\n",
    "                       combiner = self.combiner,\n",
    "                       reducer=self.reducer)\n",
    "               ]\n",
    "#    def jobconf(self):\n",
    "#        orig_jobconf = super(MRKmeansIteration, self).jobconf()        \n",
    "#        custom_jobconf = {\n",
    "#            'mapred.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "#            'mapred.text.key.comparator.options': '-k1rn',\n",
    "#            'mapred.reduce.tasks': '1',\n",
    "#        }\n",
    "#        combined_jobconf = orig_jobconf\n",
    "#        combined_jobconf.update(custom_jobconf)\n",
    "#        self.jobconf = combined_jobconf\n",
    "#        return combined_jobconf\n",
    "\n",
    "    def configure_options(self):\n",
    "        \"\"\"Add command-line options specific to this script.\"\"\"\n",
    "        super(MRKmeansIteration, self).configure_options()\n",
    "\n",
    "        self.add_passthrough_option(\n",
    "            '--centroidsFile', dest='centroidsFile', default=\"Centroids.txt\", type='str',#type='int',\n",
    "            help=('The location of the centroids file that contains the centroids '\n",
    "                  ' that will be used for this iteration of KMeans Clustering. Default: %Centroids.txt'))\n",
    "\n",
    "    #load centroids info from file\n",
    "    def mapper_init(self):\n",
    "        print \"Current path:\", os.path.dirname(os.path.realpath(__file__))\n",
    "        self.centroid_points = [map(float,s.split('\\n')[0].split(',')) for s in open(self.options.centroidsFile).readlines()]\n",
    "        print \"Centroids: \", self.centroid_points\n",
    "        \n",
    "    #load data and output the nearest centroid index and data point \n",
    "    def mapper(self, _, line):\n",
    "        D = (map(float,line.split(',')))\n",
    "        \n",
    "        w = 1/(sqrt(D[0]**2 + D[1]**2)) # compute weight\n",
    "        yield int(MinDist(D,self.centroid_points)), (D[0]*w,D[1]*w,w) # Change to weight\n",
    "        \n",
    "    #Combine sum of data points locally\n",
    "    def combiner(self, idx, inputdata):\n",
    "        sumx = sumy = num = 0\n",
    "        for x,y,n in inputdata:\n",
    "            num = num + n\n",
    "            sumx = sumx + x\n",
    "            sumy = sumy + y\n",
    "        yield idx,(sumx,sumy,num)\n",
    "        \n",
    "    #Aggregate sum for each cluster and then calculate the new centroids\n",
    "    def reducer(self, idx, inputdata): \n",
    "        sumx = sumy = num = 0\n",
    "        for x,y,n in inputdata:\n",
    "            num = num + n\n",
    "            sumx = sumx + x\n",
    "            sumy = sumy + y\n",
    "        # send the centroids back to the driver\n",
    "        yield idx,(sumx/num, sumy/num)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRKmeansIteration.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Centroids.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile Centroids.txt\n",
    "1.88191837648,0.761374538163\n",
    "0.0194073517331,2.92156922133\n",
    "0.334820407262,0.480496801461"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.88191837648,0.761374538163\r\n",
      "0.0194073517331,2.92156922133\r\n",
      "0.334820407262,0.480496801461"
     ]
    }
   ],
   "source": [
    "!head Centroids.txt   #fake cluster centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /var/folders/r5/xnt6t6nd343fqlxyqjxqz3n50qpq8v/T/MRKmeansIteration.z084224.20160711.181758.485317\n",
      "Running step 1 of 1...\n",
      "Current path: /Users/z084224/Downloads\n",
      "Centroids:  [[0.0935029243061, 0.0522456721659], [2.50120601597, 2.57660018845], [0.0407462704218, 0.0874330548455]]\n",
      "Current path: /Users/z084224/Downloads\n",
      "Centroids:  [[0.0935029243061, 0.0522456721659], [2.50120601597, 2.57660018845], [0.0407462704218, 0.0874330548455]]\n",
      "Streaming final output from /var/folders/r5/xnt6t6nd343fqlxyqjxqz3n50qpq8v/T/MRKmeansIteration.z084224.20160711.181758.485317/output...\n",
      "0\t[2.759601420725678, -0.8601607205238444]\n",
      "1\t[2.972166472948007, 3.0350687877820035]\n",
      "2\t[-0.8842684642878555, 2.6515808763000153]\n",
      "Removing temp directory /var/folders/r5/xnt6t6nd343fqlxyqjxqz3n50qpq8v/T/MRKmeansIteration.z084224.20160711.181758.485317...\n"
     ]
    }
   ],
   "source": [
    "#Test an iteration of KMeans\n",
    "!python MRKmeansIteration.py --centroidsFile=Centroids.txt --file=Centroids.txt Kmeandata.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting KMeans Iteration 0:\n",
      "Current path: /private/var/folders/r5/xnt6t6nd343fqlxyqjxqz3n50qpq8v/T/MRKmeansIteration.z084224.20160711.181803.445687/job_local_dir/0/mapper/0\n",
      "Centroids:  [[-2.47308746955, -0.698071096448], [0.0433718238941, 1.48315312101], [-2.64057583265, 2.64423764392]]\n",
      "Current path: /private/var/folders/r5/xnt6t6nd343fqlxyqjxqz3n50qpq8v/T/MRKmeansIteration.z084224.20160711.181803.445687/job_local_dir/0/mapper/1\n",
      "Centroids:  [[-2.47308746955, -0.698071096448], [0.0433718238941, 1.48315312101], [-2.64057583265, 2.64423764392]]\n",
      "\n",
      "\n",
      "Starting KMeans Iteration 1:\n",
      "Current path: /private/var/folders/r5/xnt6t6nd343fqlxyqjxqz3n50qpq8v/T/MRKmeansIteration.z084224.20160711.181804.907908/job_local_dir/0/mapper/0\n",
      "Centroids:  [[-0.109899942865, -1.17736476124], [3.11674276185, 2.4068664936], [-1.01491734809, 4.26519521422]]\n",
      "Current path: /private/var/folders/r5/xnt6t6nd343fqlxyqjxqz3n50qpq8v/T/MRKmeansIteration.z084224.20160711.181804.907908/job_local_dir/0/mapper/1\n",
      "Centroids:  [[-0.109899942865, -1.17736476124], [3.11674276185, 2.4068664936], [-1.01491734809, 4.26519521422]]\n",
      "\n",
      "\n",
      "Starting KMeans Iteration 2:\n",
      "Current path: /private/var/folders/r5/xnt6t6nd343fqlxyqjxqz3n50qpq8v/T/MRKmeansIteration.z084224.20160711.181806.350412/job_local_dir/0/mapper/0\n",
      "Centroids:  [[2.36496771298, -0.828118339948], [4.38338282123, 2.28865719751], [-0.193868569726, 3.85991533632]]\n",
      "Current path: /private/var/folders/r5/xnt6t6nd343fqlxyqjxqz3n50qpq8v/T/MRKmeansIteration.z084224.20160711.181806.350412/job_local_dir/0/mapper/1\n",
      "Centroids:  [[2.36496771298, -0.828118339948], [4.38338282123, 2.28865719751], [-0.193868569726, 3.85991533632]]\n",
      "\n",
      "\n",
      "Starting KMeans Iteration 3:\n",
      "Current path: /private/var/folders/r5/xnt6t6nd343fqlxyqjxqz3n50qpq8v/T/MRKmeansIteration.z084224.20160711.181807.909142/job_local_dir/0/mapper/0\n",
      "Centroids:  [[3.38695447069, -0.424534741278], [5.22312490107, 3.7636172428], [-0.0243559173337, 3.76526370992]]\n",
      "Current path: /private/var/folders/r5/xnt6t6nd343fqlxyqjxqz3n50qpq8v/T/MRKmeansIteration.z084224.20160711.181807.909142/job_local_dir/0/mapper/1\n",
      "Centroids:  [[3.38695447069, -0.424534741278], [5.22312490107, 3.7636172428], [-0.0243559173337, 3.76526370992]]\n",
      "\n",
      "\n",
      "Starting KMeans Iteration 4:\n",
      "Current path: /private/var/folders/r5/xnt6t6nd343fqlxyqjxqz3n50qpq8v/T/MRKmeansIteration.z084224.20160711.181809.482629/job_local_dir/0/mapper/0\n",
      "Centroids:  [[3.70367393221, -0.0696597976975], [5.79762577222, 5.60281321638], [-0.00730179644821, 3.73375193037]]\n",
      "Current path: /private/var/folders/r5/xnt6t6nd343fqlxyqjxqz3n50qpq8v/T/MRKmeansIteration.z084224.20160711.181809.482629/job_local_dir/0/mapper/1\n",
      "Centroids:  [[3.70367393221, -0.0696597976975], [5.79762577222, 5.60281321638], [-0.00730179644821, 3.73375193037]]\n",
      "\n",
      "\n",
      "Starting KMeans Iteration 5:\n",
      "Current path: /private/var/folders/r5/xnt6t6nd343fqlxyqjxqz3n50qpq8v/T/MRKmeansIteration.z084224.20160711.181810.998496/job_local_dir/0/mapper/0\n",
      "Centroids:  [[3.74083193387, 0.0014859081627], [5.9051335526, 5.90571914346], [0.000467883825641, 3.73103202139]]\n",
      "Current path: /private/var/folders/r5/xnt6t6nd343fqlxyqjxqz3n50qpq8v/T/MRKmeansIteration.z084224.20160711.181810.998496/job_local_dir/0/mapper/1\n",
      "Centroids:  [[3.74083193387, 0.0014859081627], [5.9051335526, 5.90571914346], [0.000467883825641, 3.73103202139]]\n",
      "\n",
      "\n",
      "Centroids\n",
      "\n",
      "[[3.7420380545400356, 0.0033184148501329527], [5.906700174042578, 5.911135627771637], [0.0008903978643809166, 3.7311501252066823]]\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from itertools import chain\n",
    "from numpy import random\n",
    "from MRKmeansIteration import MRKmeansIteration\n",
    "\n",
    "\n",
    "#Check whether the centroids have converged\n",
    "def stop_criterion(centroid_points_old, centroid_points_new,T):\n",
    "    oldvalue = list(chain(*centroid_points_old))\n",
    "    newvalue = list(chain(*centroid_points_new))\n",
    "    Diff = [abs(x-y) for x, y in zip(oldvalue, newvalue)]\n",
    "    Flag = True\n",
    "    for i in Diff:\n",
    "        if(i>T):\n",
    "            Flag = False\n",
    "            break\n",
    "    return Flag\n",
    "\n",
    "#Generate initial centroids\n",
    "centroid_points = []\n",
    "k = 3\n",
    "# the file that is broadcast to the workers before each iteration is started\n",
    "centroidsFile = 'Centroids.txt'  \n",
    "for i in range(k):\n",
    "    centroid_points.append([random.uniform(-3,3),random.uniform(-3,3)])\n",
    "with open(centroidsFile, 'w+') as f:\n",
    "        f.writelines(','.join(str(j) for j in i) + '\\n' for i in centroid_points)\n",
    "\n",
    "\n",
    "# Create an instance of the KMeansIteration class\n",
    "# This does all the work of assigning the data to centroids and recalculating the centroids \n",
    "# before shipping them back to the driver\n",
    "#mr_job = MRKmeansIteration(args=['Kmeandata.csv', '--file=Centroids.txt',  '--k=3' ])\n",
    "mr_job = MRKmeansIteration(args=['Kmeandata.csv', '--centroidsFile='+centroidsFile, '--file='+centroidsFile])\n",
    "\n",
    "# Update centroids iteratively\n",
    "i = 0\n",
    "while(1):\n",
    "    # save previous centoids to check convergency\n",
    "    centroid_points_old = centroid_points[:]\n",
    "    print \"Starting KMeans Iteration \"+str(i)+\":\"\n",
    "\n",
    "    with mr_job.make_runner() as runner: \n",
    "        runner.run()\n",
    "\n",
    "        # stream_output: get access of the output \n",
    "        for line in runner.stream_output():\n",
    "            key,value =  mr_job.parse_output_line(line)\n",
    "            centroid_points[key] = value\n",
    "\n",
    "        # Update the centroids for the next iteration\n",
    "        with open(centroidsFile, 'w') as f:\n",
    "            f.writelines(','.join(str(j) for j in i) + '\\n' for i in centroid_points)\n",
    "\n",
    "        print \"\\n\"\n",
    "        i = i + 1\n",
    "        if(stop_criterion(centroid_points_old,centroid_points,0.01)):\n",
    "            break\n",
    "print \"Centroids\\n\"\n",
    "print centroid_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMEANS Questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MT11. Which result below is the closest to the centroids you got after running your weighted K-means code for 10 iterations?   \n",
    "\n",
    "(a) (-4.0,0.0), (4.0,0.0), (6.0,6.0)   \n",
    "(b) (-4.5,0.0), (4.5,0.0), (0.0,4.5)  \n",
    "(c) (-5.5,0.0), (0.0,0.0), (3.0,3.0)   \n",
    "(d) (-4.5,0.0), (-4.0,0.0), (0.0,4.5)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer MT11\n",
    "A\n",
    "\n",
    "Actual answers were (0,4), (6,6) (4,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MT12. Using the result of the previous question, which number below is the closest  to the average weighted distance between each example and its assigned (closest) centroid?\n",
    "The average weighted distance is defined as \n",
    "* sum over each data point i  (weight_i * MinDist(datapoint, centroid_points) )     /  (sum over i (weight_i))\n",
    "\n",
    "where *weight_i* is defined as above:\n",
    "\n",
    "* weight_i = weight(Xi)= 1/||Xi||, \n",
    "\n",
    "where ||Xi|| = SQRT(Xi.Xi)= SQRT(x1_i^2 + x2_i^2)\n",
    "\n",
    "\n",
    "(a) 2.5   \n",
    "(b) 1.5   \n",
    "(c) 0.5   \n",
    "(d) 4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer MT12\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===Map-Reduce===\n",
    "\n",
    "### MT13. Which of the following statements are true? Select all that apply.    \n",
    " (a)\t  Since K-Means is an unsupervised learning algorithm, it cannot overfit the data, and thus it is always better to have as large a number of clusters as is computationally feasible.    \n",
    " (this is jus one way. you can also use random initialization) \n",
    " (b)\t  The standard way of initializing K-means is setting μ1=⋯=μk to be equal to a vector of zeros.    \n",
    " (c)\t  For some datasets, the \"right\" or \"correct\" value of K (the number of clusters) can be ambiguous, and hard even for a human expert looking carefully at the data to decide.    \n",
    " (d)\t  A good way to initialize K-means is to select uniformly at random K (distinct) examples from the training set and set the cluster centroids equal to these selected examples.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Answer MT13\n",
    "C, D are true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MT14. Is there a map input format (for Hadoop or MRJob)?   \n",
    "\n",
    "(a)  Yes, but only in Hadoop 0.22+.   \n",
    "(b)  Yes, in Hadoop there is a default expectation that each record is delimited by an end of line charcacter and that key is the first token delimited by a tab character and that the value-part  is everything after the tab character.   \n",
    "(c)  No,  when  MRJob INPUT_PROTOCOL = RawValueProtocol. In this case input is processed in format agnostic way thereby avoiding any type of parsing errors. The value is treated as a str, the key is read in as None.   \n",
    "(d) Both b and c are correct answers.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Answer MT14\n",
    "D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MT15. What happens if mapper output does not match reducer input?   \n",
    " \n",
    "(a)  Hadoop API will convert the data to the type that is needed by the reducer.    \n",
    "(b)  Data input/output inconsistency cannot occur. A preliminary validation check is executed prior to the full execution of the job to ensure there is consistency.    \n",
    "(c)  The java compiler will report an error during compilation but the job will complete with exceptions.    \n",
    "(d)  A real-time exception will be thrown and map-reduce job will fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Answer MT15\n",
    "D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### MT16. Why would a developer create a map-reduce without the reduce step?   \n",
    " \n",
    "(a)  Developers should design Map-Reduce jobs without reducers only if no reduce slots are available on the cluster.    \n",
    "(b)  Developers should never design Map-Reduce jobs without reducers. An error will occur upon compile.    \n",
    "(c)  There is a CPU intensive step that occurs between the map and reduce steps. Disabling the reduce step speeds up data processing.  \n",
    "(d)  It is not possible to create a map-reduce job without at least one reduce step. A developer may decide to limit to one reducer for debugging purposes.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer MT16\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# END of Exam"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
